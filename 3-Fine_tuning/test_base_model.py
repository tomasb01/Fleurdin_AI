"""
TEST BASE MODEL (PÅ˜ED FINE-TUNINGEM)
=====================================

Tento skript testuje base Gemma 2-2B-it model PÅ˜ED fine-tuningem
na 10 testovacÃ­ch otÃ¡zkÃ¡ch z test_questions.txt

VÃ½sledky uloÅ¾Ã­ do: results_before_finetuning.txt
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datetime import datetime

# ============================================
# KONFIGURACE
# ============================================

MODEL_NAME = "google/gemma-2-2b-it"
TEST_QUESTIONS_FILE = "../1-EO_Dataset/test_questions.txt"
OUTPUT_FILE = "results_before_finetuning.txt"

# TestovacÃ­ otÃ¡zky (z test_questions.txt - aktualizovÃ¡no 2025-01-30)
TEST_QUESTIONS = [
    "JakÃ© jsou ÃºÄinky oregana na tÄ›lo?",
    "JakÃ© oleje bys doporuÄil na psychickou Ãºnavu a stres?",
    "KterÃ© esenciÃ¡lnÃ­ oleje pomÃ¡hajÃ­ pÅ™i zaÅ¾Ã­vacÃ­ch obtÃ­Å¾Ã­ch?",
    "PomÅ¯Å¾e levandule pÅ™i nespavosti?",
    "Na co se pouÅ¾Ã­vÃ¡ mÃ¡ta peprnÃ¡?",
    "K Äemu je u oleje uvedenÃ¡ jeho frekvence?",
    "JakÃ© esenciÃ¡lnÃ­ oleje bys doporuÄil na bolesti kloubÅ¯ a svalÅ¯?",
    "KterÃ½ olej je dobrÃ½ na trÃ¡venÃ­?",
    "Jak bys vytvoÅ™il smÄ›s olejÅ¯ na podporu spÃ¡nku?"
]

# System prompt (stejnÃ½ jako budeme pouÅ¾Ã­vat po fine-tuningu)
SYSTEM_PROMPT = """Jsi expert na esenciÃ¡lnÃ­ oleje a aromaterapii.
Poskytuj pÅ™esnÃ©, odbornÃ© informace o ÃºÄincÃ­ch esenciÃ¡lnÃ­ch olejÅ¯ na tÄ›lo a psychiku.
OdpovÃ­dej v ÄeÅ¡tinÄ›, struÄnÄ› a srozumitelnÄ›."""


# ============================================
# FUNKCE
# ============================================

def load_model():
    """NaÄti base model a tokenizer"""
    print(f"ğŸ“¥ NaÄÃ­tÃ¡m model: {MODEL_NAME}")
    print("âš ï¸  To mÅ¯Å¾e trvat nÄ›kolik minut...")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto",
        torch_dtype=torch.bfloat16  # Ãšspora pamÄ›ti
    )

    print("âœ… Model naÄten!")
    return tokenizer, model


def test_question(question, tokenizer, model):
    """Testuj jednu otÃ¡zku"""

    # FormÃ¡tovÃ¡nÃ­ pro Gemma 2-2B-it (chat format)
    messages = [
        {"role": "user", "content": f"{SYSTEM_PROMPT}\n\n{question}"}
    ]

    # Tokenizace
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    # GenerovÃ¡nÃ­ odpovÄ›di
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )

    # DekÃ³dovÃ¡nÃ­
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extrakce jen odpovÄ›di (bez promptu)
    # Gemma formÃ¡t: <start_of_turn>user\n...<end_of_turn>\n<start_of_turn>model\nODPOVÄšÄ
    if "<start_of_turn>model" in response:
        answer = response.split("<start_of_turn>model")[-1].strip()
    else:
        answer = response

    return answer


def run_tests():
    """SpusÅ¥ vÅ¡echny testy"""

    print("\n" + "="*60)
    print("ğŸ§ª TEST BASE MODEL (PÅ˜ED FINE-TUNINGEM)")
    print("="*60 + "\n")

    # NaÄti model
    tokenizer, model = load_model()

    # OtevÅ™i output file
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("TEST BASE MODEL (PÅ˜ED FINE-TUNINGEM)\n")
        f.write("="*60 + "\n\n")
        f.write(f"Model: {MODEL_NAME}\n")
        f.write(f"Datum: {timestamp}\n")
        f.write(f"PoÄet otÃ¡zek: {len(TEST_QUESTIONS)}\n")
        f.write("\n" + "="*60 + "\n\n")

        # Testuj kaÅ¾dou otÃ¡zku
        for i, question in enumerate(TEST_QUESTIONS, 1):
            print(f"\nğŸ“ OtÃ¡zka {i}/{len(TEST_QUESTIONS)}: {question}")

            answer = test_question(question, tokenizer, model)

            print(f"ğŸ’¬ OdpovÄ›Ä: {answer[:100]}...")  # Preview

            # UloÅ¾ do souboru
            f.write(f"OTÃZKA {i}:\n")
            f.write(f"{question}\n\n")
            f.write(f"ODPOVÄšÄ:\n")
            f.write(f"{answer}\n")
            f.write("\n" + "-"*60 + "\n\n")

    print(f"\nâœ… Hotovo! VÃ½sledky uloÅ¾eny do: {OUTPUT_FILE}")
    print("\nğŸ’¡ Porovnej tyto odpovÄ›di s odpovÄ›Ämi PO fine-tuningu.")


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    run_tests()
