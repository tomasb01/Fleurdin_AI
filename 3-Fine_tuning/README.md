# ğŸŒ¿ FLEURDIN - FINE-TUNING GEMMA 2-2B-IT

Fine-tuning Gemma 2-2B-it modelu na datasetu **TomasBo/Fleurdin** (2,281 Q&A pÃ¡rÅ¯ o esenciÃ¡lnÃ­ch olejÃ­ch).

---

## ğŸ“‹ OBSAH

1. [Hardware poÅ¾adavky](#hardware-poÅ¾adavky)
2. [Instalace](#instalace)
3. [Workflow](#workflow)
4. [SpuÅ¡tÄ›nÃ­](#spuÅ¡tÄ›nÃ­)
5. [VÃ½sledky](#vÃ½sledky)
6. [Troubleshooting](#troubleshooting)

---

## ğŸ’» HARDWARE POÅ½ADAVKY

### **MinimÃ¡lnÃ­:**
- GPU: 12GB VRAM (RTX 3060, RTX 4060 Ti, T4)
- RAM: 16GB
- Disk: 20GB free space

### **DoporuÄenÃ©:**
- GPU: 16GB+ VRAM (RTX 4080, A10G, A100)
- RAM: 32GB
- Disk: 50GB free

### **Alternativy:**
- **Google Colab** (free tier s T4 GPU) âœ… DOPORUÄŒENO pro zaÄÃ¡tek
- **Kaggle Notebooks** (free P100 GPU)
- **RunPod / Vast.ai** (pronÃ¡jem GPU od $0.30/hodina)

---

## ğŸ“¦ INSTALACE

### **1. Klonuj repozitÃ¡Å™ (nebo jsi uÅ¾ v nÄ›m)**

```bash
cd C:\Projects\Fleurdin_AI\2-Fine_tuning
```

### **2. VytvoÅ™ virtual environment**

```bash
# Python 3.10+ required
python -m venv venv

# Aktivuj
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate
```

### **3. Nainstaluj dependencies**

```bash
pip install -r requirements.txt
```

**âš ï¸ DÅ®LEÅ½ITÃ‰:** Pokud mÃ¡Å¡ **NVIDIA GPU**, nainstaluj CUDA-enabled PyTorch:

```bash
# CUDA 11.8
pip install torch --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

### **4. OvÄ›Å™ GPU**

```bash
python -c "import torch; print(torch.cuda.is_available())"
# MÄ›lo by vrÃ¡tit: True
```

---

## ğŸ”„ WORKFLOW

```
1. TEST BASE MODEL (PÅ˜ED fine-tuningem)
   â†“
   python test_base_model.py
   â†’ VÃ½stup: results_before_finetuning.txt

2. FINE-TUNING
   â†“
   python finetune_gemma.py
   â†’ Training (1-3 hodiny)
   â†’ Model uloÅ¾en do: ./fleurdin-gemma-2b/

3. TEST FINE-TUNED MODEL (PO fine-tuningu)
   â†“
   python test_finetuned_model.py
   â†’ VÃ½stup: results_after_finetuning.txt

4. POROVNÃNÃ
   â†“
   Porovnej results_before_finetuning.txt vs results_after_finetuning.txt
```

---

## ğŸš€ SPUÅ TÄšNÃ

### **KROK 1: Test base modelu (baseline)**

```bash
python test_base_model.py
```

**Co to dÄ›lÃ¡:**
- NaÄte base Gemma 2-2B-it (bez fine-tuningu)
- ZeptÃ¡ se 9 testovacÃ­ch otÃ¡zek
- UloÅ¾Ã­ odpovÄ›di do `results_before_finetuning.txt`

**ÄŒas:** ~5-10 minut (stahovÃ¡nÃ­ modelu + inference)

---

### **KROK 2: Fine-tuning**

```bash
python finetune_gemma.py
```

**Co to dÄ›lÃ¡:**
- NaÄte dataset `TomasBo/Fleurdin` (2,281 Q&A)
- NaÄte Gemma 2-2B-it v 4-bit quantization
- PÅ™idÃ¡ LoRA adapters (parameter-efficient)
- TrÃ©nuje 3 epochy
- UloÅ¾Ã­ model do `./fleurdin-gemma-2b/`

**Parametry (mÅ¯Å¾eÅ¡ upravit v `finetune_gemma.py`):**
```python
TRAINING_CONFIG = {
    "num_train_epochs": 3,           # PoÄet epoch
    "per_device_train_batch_size": 4,
    "learning_rate": 2e-4,
}
```

**ÄŒas:** 1-3 hodiny (zÃ¡leÅ¾Ã­ na GPU)

**PrÅ¯bÄ›Å¾nÃ© logy:**
```
Epoch 1/3:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 100/300 [15:20<30:40,  9.20s/it]
```

**âš ï¸ Pokud dojde pamÄ›Å¥ (OOM):**
- SniÅ¾ `per_device_train_batch_size` na 2 nebo 1
- SniÅ¾ `max_length` z 512 na 256

---

### **KROK 3: Test fine-tuned modelu**

```bash
python test_finetuned_model.py
```

**Co to dÄ›lÃ¡:**
- NaÄte fine-tuned model z `./fleurdin-gemma-2b/`
- ZeptÃ¡ se stejnÃ½ch 9 otÃ¡zek
- UloÅ¾Ã­ odpovÄ›di do `results_after_finetuning.txt`

**ÄŒas:** ~2-5 minut

---

### **KROK 4: PorovnÃ¡nÃ­ vÃ½sledkÅ¯**

OtevÅ™i oba soubory vedle sebe:
- `results_before_finetuning.txt`
- `results_after_finetuning.txt`

**OÄekÃ¡vanÃ© zlepÅ¡enÃ­:**
âœ… PÅ™esnÄ›jÅ¡Ã­ odpovÄ›di (konkrÃ©tnÃ­ oleje)
âœ… LepÅ¡Ã­ terminologie (aromaterapie)
âœ… StruÄnÄ›jÅ¡Ã­, strukturovanÄ›jÅ¡Ã­ text
âœ… SprÃ¡vnÃ© odpovÄ›di na frekvence

---

## ğŸ“Š VÃSLEDKY

### **TestovacÃ­ otÃ¡zky:**

1. JakÃ© jsou ÃºÄinky oregana na tÄ›lo?
2. JakÃ© oleje bys doporuÄil na psychickou Ãºnavu a stres?
3. KterÃ© esenciÃ¡lnÃ­ oleje pomÃ¡hajÃ­ pÅ™i zaÅ¾Ã­vacÃ­ch obtÃ­Å¾Ã­ch?
4. PomÅ¯Å¾e levandule pÅ™i nespavosti?
5. Na co se pouÅ¾Ã­vÃ¡ mÃ¡ta peprnÃ¡?
6. K Äemu je u oleje uvedenÃ¡ jeho frekvence?
7. JakÃ© esenciÃ¡lnÃ­ oleje bys doporuÄil na bolesti kloubÅ¯ a svalÅ¯?
8. KterÃ½ olej je dobrÃ½ na trÃ¡venÃ­?
9. Jak bys vytvoÅ™il smÄ›s olejÅ¯ na podporu spÃ¡nku?

### **Metriky k hodnocenÃ­:**

| Aspekt | Base model | Fine-tuned model |
|---|---|---|
| **PÅ™esnost** | ObecnÃ© odpovÄ›di | KonkrÃ©tnÃ­ oleje z datasetu |
| **Terminologie** | StandardnÃ­ | Aromaterapie specifickÃ¡ |
| **Styl** | GenerickÃ½ | Expert on EO |
| **FaktiÄnost** | MoÅ¾nÃ© halucinace | Fakta z datasetu |

---

## ğŸ“¤ UPLOAD NA HUGGINGFACE

Po ÃºspÄ›Å¡nÃ©m fine-tuningu:

```bash
# V Python konzoli nebo scriptu
from huggingface_hub import login
login()  # Zadej svÅ¯j HF token

# Upload
model.push_to_hub("TomasBo/fleurdin-gemma-2b")
tokenizer.push_to_hub("TomasBo/fleurdin-gemma-2b")
```

Nebo v `finetune_gemma.py` odpovÄ›z "y" na vÃ½zvu k uploadu.

---

## ğŸ› TROUBLESHOOTING

### **ProblÃ©m: CUDA out of memory**

```
RuntimeError: CUDA out of memory
```

**Å˜eÅ¡enÃ­:**
1. SniÅ¾ batch size v `finetune_gemma.py`:
   ```python
   "per_device_train_batch_size": 2  # nebo 1
   ```

2. SniÅ¾ max_length:
   ```python
   max_length=256  # mÃ­sto 512
   ```

3. PouÅ¾ij gradient checkpointing (uÅ¾ je enabled v LoRA)

---

### **ProblÃ©m: Model se nestahuje**

```
ConnectionError: Failed to download model
```

**Å˜eÅ¡enÃ­:**
1. OvÄ›Å™ internet spojenÃ­
2. Login do HuggingFace:
   ```bash
   huggingface-cli login
   ```

3. Zkus ruÄnÄ› stÃ¡hnout:
   ```python
   from transformers import AutoModelForCausalLM
   model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b-it")
   ```

---

### **ProblÃ©m: Slow training**

```
Training trvÃ¡ 10+ hodin
```

**Å˜eÅ¡enÃ­:**
1. OvÄ›Å™ Å¾e pouÅ¾Ã­vÃ¡Å¡ GPU:
   ```python
   import torch
   print(torch.cuda.is_available())  # MÄ›lo by bÃ½t True
   ```

2. Zkontroluj GPU vyuÅ¾itÃ­:
   ```bash
   nvidia-smi
   ```

3. PouÅ¾ij mixed precision (uÅ¾ enabled via bf16)

---

### **ProblÃ©m: Fine-tuned model je horÅ¡Ã­ neÅ¾ base**

**MoÅ¾nÃ© pÅ™Ã­Äiny:**
1. **Overfitting** - zkus mÃ©nÄ› epoch (1-2 mÃ­sto 3)
2. **Learning rate moc vysokÃ½** - zkus 1e-4 mÃ­sto 2e-4
3. **Dataset issues** - zkontroluj kvalitu datasetu

**Å˜eÅ¡enÃ­:**
```python
# Upravit v finetune_gemma.py
"num_train_epochs": 2,  # mÃ­sto 3
"learning_rate": 1e-4,  # mÃ­sto 2e-4
```

---

## ğŸ¯ NEXT STEPS (po fine-tuningu)

1. âœ… Porovnej vÃ½sledky (before vs after)
2. âœ… Upload na HuggingFace: `TomasBo/fleurdin-gemma-2b`
3. âœ… Deploy Inference Endpoint (HuggingFace nebo Modal.com)
4. âœ… Integrace do Fleurdin architektury (RAG pipeline)

---

## ğŸ“š RESOURCES

- **Dataset:** https://huggingface.co/datasets/TomasBo/Fleurdin
- **Base model:** https://huggingface.co/google/gemma-2-2b-it
- **PEFT docs:** https://huggingface.co/docs/peft
- **Gemma fine-tuning guide:** https://huggingface.co/blog/gemma-peft

---

**VytvoÅ™eno:** 2025-01-30
**Pro:** Fleurdin AI
**Kontakt:** [@TomasBo](https://huggingface.co/TomasBo)
