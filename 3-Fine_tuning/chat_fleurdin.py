#!/usr/bin/env python3
"""
ğŸŒ¿ Fleurdin AI - Chat CLI
PovÃ­dej si s fine-tuned modelem pÅ™Ã­mo z terminÃ¡lu!
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM
import torch

print("ğŸŒ¿ NaÄÃ­tÃ¡m Fleurdin AI model...")
print("(PrvnÃ­ spuÅ¡tÄ›nÃ­ mÅ¯Å¾e trvat ~5 min - stahovÃ¡nÃ­ modelu)")
print("-" * 60)

# Load model with LoRA adapters
model = AutoPeftModelForCausalLM.from_pretrained(
    "TomasBo/Essention_oils-Mistral-7B-Instruct-v0.3-lora-adapter",
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "TomasBo/Essention_oils-Mistral-7B-Instruct-v0.3-lora-adapter"
)

print("âœ… Model naÄten!")
print("\nğŸŒ¿ Fleurdin AI Chat")
print("Zeptej se na esenciÃ¡lnÃ­ oleje (Ctrl+C pro ukonÄenÃ­)")
print("-" * 60)

conversation_history = []

while True:
    try:
        # Get user input
        user_input = input("\nğŸ’¬ Ty: ")

        if not user_input.strip():
            continue

        # Build prompt with conversation history
        messages = conversation_history + [
            {"role": "user", "content": user_input}
        ]

        # Format with chat template
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract only the assistant's response
        if "[/INST]" in response:
            assistant_response = response.split("[/INST]")[-1].strip()
        else:
            assistant_response = response[len(prompt):].strip()

        print(f"\nğŸ¤– Fleurdin AI: {assistant_response}")

        # Update conversation history (keep last 3 exchanges)
        conversation_history.append({"role": "user", "content": user_input})
        conversation_history.append({"role": "assistant", "content": assistant_response})

        if len(conversation_history) > 6:  # Keep last 3 exchanges
            conversation_history = conversation_history[-6:]

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Nashledanou!")
        break
    except Exception as e:
        print(f"\nâŒ Chyba: {e}")
        continue
