"""
TEST FINE-TUNED MODEL (PO FINE-TUNINGU)
========================================

Tento skript testuje fine-tuned Gemma 2-2B-it model PO fine-tuningu
na stejnÃ½ch 9 testovacÃ­ch otÃ¡zkÃ¡ch.

VÃ½sledky uloÅ¾Ã­ do: results_after_finetuning.txt
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from datetime import datetime

# ============================================
# KONFIGURACE
# ============================================

BASE_MODEL = "google/gemma-2-2b-it"
FINETUNED_MODEL_PATH = "./fleurdin-gemma-2b"  # LokÃ¡lnÃ­ cesta k LoRA adapters
OUTPUT_FILE = "results_after_finetuning.txt"

# TestovacÃ­ otÃ¡zky (stejnÃ© jako v test_base_model.py - aktualizovÃ¡no 2025-01-30)
TEST_QUESTIONS = [
    "JakÃ© jsou ÃºÄinky oregana na tÄ›lo?",
    "JakÃ© oleje bys doporuÄil na psychickou Ãºnavu a stres?",
    "KterÃ© esenciÃ¡lnÃ­ oleje pomÃ¡hajÃ­ pÅ™i zaÅ¾Ã­vacÃ­ch obtÃ­Å¾Ã­ch?",
    "PomÅ¯Å¾e levandule pÅ™i nespavosti?",
    "Na co se pouÅ¾Ã­vÃ¡ mÃ¡ta peprnÃ¡?",
    "K Äemu je u oleje uvedenÃ¡ jeho frekvence?",
    "JakÃ© esenciÃ¡lnÃ­ oleje bys doporuÄil na bolesti kloubÅ¯ a svalÅ¯?",
    "KterÃ½ olej je dobrÃ½ na trÃ¡venÃ­?",
    "Jak bys vytvoÅ™il smÄ›s olejÅ¯ na podporu spÃ¡nku?"
]

# System prompt
SYSTEM_PROMPT = """Jsi expert na esenciÃ¡lnÃ­ oleje a aromaterapii.
Poskytuj pÅ™esnÃ©, odbornÃ© informace o ÃºÄincÃ­ch esenciÃ¡lnÃ­ch olejÅ¯ na tÄ›lo a psychiku.
OdpovÃ­dej v ÄeÅ¡tinÄ›, struÄnÄ› a srozumitelnÄ›."""


# ============================================
# FUNKCE
# ============================================

def load_finetuned_model():
    """NaÄti fine-tuned model (base + LoRA adapters)"""

    print(f"ğŸ“¥ NaÄÃ­tÃ¡m base model: {BASE_MODEL}")
    print("âš ï¸  To mÅ¯Å¾e trvat nÄ›kolik minut...")

    # NaÄti base model
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        torch_dtype=torch.bfloat16
    )

    # NaÄti LoRA adapters (fine-tuned weights)
    print(f"ğŸ“¥ NaÄÃ­tÃ¡m fine-tuned adapters z: {FINETUNED_MODEL_PATH}")
    model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)

    # Merge adapters do base model (pro rychlejÅ¡Ã­ inference)
    print("ğŸ”€ MergovÃ¡nÃ­ adapters...")
    model = model.merge_and_unload()

    print("âœ… Fine-tuned model naÄten!")
    return tokenizer, model


def test_question(question, tokenizer, model):
    """Testuj jednu otÃ¡zku"""

    # FormÃ¡tovÃ¡nÃ­ pro Gemma 2-2B-it
    messages = [
        {"role": "user", "content": f"{SYSTEM_PROMPT}\n\n{question}"}
    ]

    # Tokenizace
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    # GenerovÃ¡nÃ­
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )

    # DekÃ³dovÃ¡nÃ­
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extrakce odpovÄ›di
    if "<start_of_turn>model" in response:
        answer = response.split("<start_of_turn>model")[-1].strip()
    else:
        answer = response

    return answer


def run_tests():
    """SpusÅ¥ vÅ¡echny testy"""

    print("\n" + "="*60)
    print("ğŸ§ª TEST FINE-TUNED MODEL (PO FINE-TUNINGU)")
    print("="*60 + "\n")

    # NaÄti model
    tokenizer, model = load_finetuned_model()

    # OtevÅ™i output file
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("TEST FINE-TUNED MODEL (PO FINE-TUNINGU)\n")
        f.write("="*60 + "\n\n")
        f.write(f"Base model: {BASE_MODEL}\n")
        f.write(f"Fine-tuned: {FINETUNED_MODEL_PATH}\n")
        f.write(f"Datum: {timestamp}\n")
        f.write(f"PoÄet otÃ¡zek: {len(TEST_QUESTIONS)}\n")
        f.write("\n" + "="*60 + "\n\n")

        # Testuj kaÅ¾dou otÃ¡zku
        for i, question in enumerate(TEST_QUESTIONS, 1):
            print(f"\nğŸ“ OtÃ¡zka {i}/{len(TEST_QUESTIONS)}: {question}")

            answer = test_question(question, tokenizer, model)

            print(f"ğŸ’¬ OdpovÄ›Ä: {answer[:100]}...")

            # UloÅ¾
            f.write(f"OTÃZKA {i}:\n")
            f.write(f"{question}\n\n")
            f.write(f"ODPOVÄšÄ:\n")
            f.write(f"{answer}\n")
            f.write("\n" + "-"*60 + "\n\n")

    print(f"\nâœ… Hotovo! VÃ½sledky uloÅ¾eny do: {OUTPUT_FILE}")
    print("\nğŸ’¡ Porovnej s results_before_finetuning.txt")


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    run_tests()
