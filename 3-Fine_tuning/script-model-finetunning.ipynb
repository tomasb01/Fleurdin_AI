{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618f55cf",
   "metadata": {},
   "source": [
    "# Trained on RunPod.io\n",
    "\n",
    "- GPU - RTX 3090 24GB / A5000 24 GB\n",
    "- RAM - 21 GB \n",
    "- HDD - 200 GB\n",
    "\n",
    "Price 0.44$/hod\n",
    "\n",
    "## 4-bit training\n",
    "\n",
    "- training took cca. 15 minutes = 0.11 $\n",
    "\n",
    "## 16-bit merged model\n",
    "\n",
    "- merge took cca. 2 minute = 0.02 $\n",
    "- push took cca. 2 minute = 0.02 $\n",
    "\n",
    "\n",
    "# Inference on TGI \n",
    "https://ui.endpoints.huggingface.co/\n",
    "\n",
    "GPU - L4 16GB VRAM\n",
    "\n",
    "Price 0.8$/hod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4a440",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "%pip install transformers\n",
    "%pip install peft\n",
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "%pip install trl\n",
    "%pip install matplotlib\n",
    "%pip install tensorboard\n",
    "%pip install sentencepiece\n",
    "%pip install hf_transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b9b6d",
   "metadata": {},
   "source": [
    "# Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "API_TOKEN = \"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\"\n",
    "login(token=API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96b83c",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84dc5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efbb9bc0",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00708b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(base_model.get_input_embeddings())\n",
    "print(base_model.get_output_embeddings())\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"before\", len(base_tokenizer))\n",
    "base_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "print(\"after\", len(base_tokenizer))\n",
    "\n",
    "print(\"before 2\", base_model.config.pad_token_id)\n",
    "base_model.config.pad_token_id = base_tokenizer.pad_token_id\n",
    "base_model.generation_config.pad_token_id = base_tokenizer.pad_token_id\n",
    "print(\"after 2\", base_model.config.pad_token_id)\n",
    "\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "base_model.resize_token_embeddings(len(base_tokenizer))\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ad212",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, tokenizer, prompt):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt as a conversation\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens (skip the input)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test 1 - simple prompt\n",
    "prompt = \"Řekni mi vtip o programátorech.\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(base_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Test 2 - Essential oils related prompt\n",
    "prompt = \"Jaká je nejvyšší známá vibrace esenciálního oleje?\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(base_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c64438",
   "metadata": {},
   "source": [
    "### Log model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d2ba0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10434ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(base_model))\n",
    "print(\"Architecture:\", base_model)\n",
    "print(\"Config:\", base_model.config)\n",
    "print(\"Generation Config:\", base_model.generation_config)\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(base_model.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(base_model.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(base_tokenizer))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", base_tokenizer.special_tokens_map)\n",
    "print(\"All tokens count:\", len(base_tokenizer))\n",
    "print(\"Padding side:\", base_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45262e6",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c02e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"TomasBo/Fleurdin\", split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.rename_column(\"text\", \"messages\") \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a7a61",
   "metadata": {},
   "source": [
    "#### Create a final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset\n",
    "final_datasets = dataset.train_test_split(test_size=0.2)\n",
    "print(final_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f572088",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df0821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ----------------------------------\n",
    "# Adding the adapters to the layers\n",
    "# ----------------------------------\n",
    "\n",
    "# PEFT\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    # target_modules=[\n",
    "    #     \"q_proj\",\n",
    "    #     \"k_proj\",\n",
    "    #     \"down_proj\",\n",
    "    #     \"v_proj\",\n",
    "    #     \"gate_proj\",\n",
    "    #     \"o_proj\",\n",
    "    #     \"up_proj\",\n",
    "    # ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # modules_to_save=[\n",
    "    #     \"lm_head\",\n",
    "    #     \"embed_tokens\",\n",
    "    # ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",  # https://huggingface.co/docs/peft/en/developer_guides/lora#qlora-style-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1a079",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e36b6",
   "metadata": {},
   "source": [
    "##### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f962369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Create timestamped run directory for this training session\n",
    "run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"run_{run_timestamp}\"\n",
    "output_dir = f\"/workspace/runs/{run_name}/model\"\n",
    "logging_dir = f\"/workspace/runs/{run_name}/logs\"\n",
    "\n",
    "print(f\"Training run: {run_name}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Logging directory: {logging_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ----------------------------------\n",
    "# Training WITH evaluation (metrics)\n",
    "# ----------------------------------\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "bs = 1  # batch size\n",
    "ga_steps = 4  # gradient acc. steps\n",
    "epochs = 5\n",
    "steps_per_epoch = len(final_datasets[\"train\"]) // (bs * ga_steps)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    learning_rate=lr,\n",
    "    save_steps=steps_per_epoch,\n",
    "    save_total_limit=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=steps_per_epoch,  # eval and save once per epoch\n",
    "    logging_steps=10,\n",
    "    logging_dir=logging_dir,\n",
    "    report_to=\"tensorboard\",  # Enable TensorBoard logging\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=10,  # Gradual warmup\n",
    "    fp16=True,\n",
    "    # bf16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    processing_class=base_tokenizer,\n",
    "    train_dataset=final_datasets[\"train\"],\n",
    "    eval_dataset=final_datasets[\"test\"],\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c2f21",
   "metadata": {},
   "source": [
    "#### Log TRAINER - Model, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Trainer model ---\")\n",
    "print(trainer.model)\n",
    "print(\"Config:\", trainer.model.config)\n",
    "print(\"Generation Config:\", trainer.model.generation_config)\n",
    "\n",
    "print(\"Get Trainable Parameters\")\n",
    "print(trainer.model.print_trainable_parameters())\n",
    "# trainable params: 167,772,160 || all params: 7,415,803,904 || trainable%: 2.2624\n",
    "\n",
    "print(\"--- Trainer tokenizer ---\")\n",
    "print(trainer.processing_class)\n",
    "print(\"Type:\", type(trainer.processing_class))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", trainer.processing_class.special_tokens_map)\n",
    "print(\"All tokens count:\", len(trainer.processing_class))\n",
    "print(\"Padding side:\", trainer.processing_class.padding_side)\n",
    "\n",
    "\n",
    "print(\"--- Trainer dataset ---\")\n",
    "print(trainer.train_dataset)\n",
    "\n",
    "for t in trainer.train_dataset[\"messages\"][:10]:\n",
    "    print(t)\n",
    "\n",
    "for t in trainer.train_dataset[\"input_ids\"][:10]:\n",
    "    print(t)\n",
    "\n",
    "\n",
    "print(\"--- Trainer data collation ---\")\n",
    "print(trainer.data_collator)\n",
    "collated_data = trainer.data_collator(trainer.train_dataset)\n",
    "print(collated_data)\n",
    "\n",
    "for t in collated_data[\"input_ids\"][:10]:\n",
    "    print(t)\n",
    "\n",
    "for t in collated_data[\"labels\"][:10]:\n",
    "    print(t)\n",
    "\n",
    "for t in collated_data[\"attention_mask\"][:10]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b044d",
   "metadata": {},
   "source": [
    "Tensorboard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbccf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOG HYPERPARAMETERS TO TENSORBOARD ---\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=logging_dir)\n",
    "\n",
    "# Create markdown summary of hyperparameters\n",
    "hyperparams_summary = f\"\"\"\n",
    "# Training Run: {run_name}\n",
    "\n",
    "## Run Information\n",
    "- **Timestamp**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- **Model**: {model_name}\n",
    "- **Dataset**: lukaskellerstein/autogen\n",
    "- **Train samples**: {len(final_datasets[\"train\"])}\n",
    "- **Eval samples**: {len(final_datasets[\"test\"])}\n",
    "\n",
    "## Model Configuration\n",
    "- **Quantization**: 4-bit (NF4)\n",
    "- **Compute dtype**: float16\n",
    "- **Double quantization**: True\n",
    "- **Base vocab size**: 32768\n",
    "- **Extended vocab size**: {base_model.config.vocab_size}\n",
    "- **Pad token ID**: {base_tokenizer.pad_token_id}\n",
    "\n",
    "## LoRA/PEFT Configuration\n",
    "- **LoRA rank (r)**: {peft_config.r}\n",
    "- **LoRA alpha**: {peft_config.lora_alpha}\n",
    "- **LoRA dropout**: {peft_config.lora_dropout}\n",
    "- **Target modules**: {peft_config.target_modules}\n",
    "- **Bias**: {peft_config.bias}\n",
    "- **Task type**: {peft_config.task_type}\n",
    "\n",
    "## Training Hyperparameters\n",
    "- **Learning rate**: {lr}\n",
    "- **Batch size**: {bs}\n",
    "- **Gradient accumulation steps**: {ga_steps}\n",
    "- **Effective batch size**: {bs * ga_steps}\n",
    "- **Epochs**: {epochs}\n",
    "- **Steps per epoch**: {steps_per_epoch}\n",
    "- **Total training steps**: {steps_per_epoch * epochs}\n",
    "- **LR scheduler**: {training_args.lr_scheduler_type}\n",
    "- **FP16**: {training_args.fp16}\n",
    "- **Eval strategy**: {training_args.eval_strategy}\n",
    "- **Eval steps**: {training_args.eval_steps}\n",
    "- **Logging steps**: {training_args.logging_steps}\n",
    "- **Save steps**: {training_args.save_steps}\n",
    "- **Save total limit**: {training_args.save_total_limit}\n",
    "\n",
    "## Directories\n",
    "- **Output dir**: {output_dir}\n",
    "- **Logging dir**: {logging_dir}\n",
    "\"\"\"\n",
    "\n",
    "writer.add_text(\"Hyperparameters\", hyperparams_summary, 0)\n",
    "writer.close()\n",
    "\n",
    "print(\"✓ Hyperparameters logged to TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee42429",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(\"Start training...\")\n",
    "startTrain = time.time()\n",
    "trainer.train()\n",
    "td = timedelta(seconds=(time.time() - startTrain))\n",
    "print(f\"Training takes: {td}\")\n",
    "\n",
    "\n",
    "# Total time for the script\n",
    "td = timedelta(seconds=(time.time() - start))\n",
    "print(f\"Total takes: {td}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016b981",
   "metadata": {},
   "source": [
    "### Test the adapter - OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79dd7a",
   "metadata": {},
   "source": [
    "# Function to test the model\n",
    "def test_model(model, tokenizer, prompt):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt as a conversation\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens (skip the input)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test 1 - simple prompt\n",
    "prompt = \"Tell me a joke about programmers.\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(base_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Test 2 - Essential oils related prompt\n",
    "prompt = \"Jaká je nejvyšší vibrace esenciálního oleje?\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(base_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a835c3c",
   "metadata": {},
   "source": [
    "### Save the adapter (to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"SAVED_ADAPTER\")\n",
    "trainer.processing_class.save_pretrained(\"SAVED_ADAPTER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d2526",
   "metadata": {},
   "source": [
    "### Push adapter (to hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2152ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.push_to_hub(\n",
    "    repo_id=\"TTomasBo/Essention_oils-Mistral-7B-Instruct-v0.3-lora-adapter\",\n",
    "    token=API_TOKEN,\n",
    ")\n",
    "trainer.processing_class.push_to_hub(\n",
    "    repo_id=\"TomasBo/Essention_oils-Mistral-7B-Instruct-v0.3-lora-adapter\",\n",
    "    token=API_TOKEN,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
