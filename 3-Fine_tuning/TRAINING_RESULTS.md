# ğŸŒ¿ Fleurdin AI - VÃ½sledky Fine-tuningu

**Datum:** 31. Å™Ã­jna 2025
**Model:** Mistral-7B-Instruct-v0.3 + LoRA adapters
**Dataset:** Fleurdin Essential Oils (2,281 Q&A pÃ¡rÅ¯)

---

## ğŸ“Š ZÃ¡kladnÃ­ informace

| Parametr | Hodnota |
|----------|---------|
| **Base Model** | mistralai/Mistral-7B-Instruct-v0.3 |
| **Fine-tuning metoda** | LoRA (Low-Rank Adaptation) |
| **Dataset** | TomasBo/Fleurdin |
| **TrÃ©novacÃ­ data** | 1,825 zÃ¡znamÅ¯ (80%) |
| **ValidaÄnÃ­ data** | 456 zÃ¡znamÅ¯ (20%) |
| **Platforma** | RunPod.io |
| **GPU** | RTX 3090 24GB |

---

## âš™ï¸ Hyperparametry

| Parametr | Hodnota |
|----------|---------|
| **PoÄet epoch** | 5 |
| **Batch size** | 1 |
| **Gradient accumulation steps** | 4 |
| **Effective batch size** | 4 |
| **Learning rate** | 0.0001 |
| **LR scheduler** | cosine |
| **Warmup steps** | 10 |
| **Precision** | FP16 |
| **Max sequence length** | 512 tokenÅ¯ |

### LoRA konfigurace:
- **LoRA rank (r):** 16
- **LoRA alpha:** 16
- **LoRA dropout:** 0.1
- **Target modules:** all-linear
- **Trainable parameters:** ~2.26% (167M / 7.4B)

---

## ğŸ“ˆ VÃ½sledky trÃ©ninku

### Training & Validation Loss

| Epoch | Step | Training Loss | Validation Loss | Mean Token Accuracy | Entropy |
|-------|------|---------------|-----------------|---------------------|---------|
| 1 | 456 | 0.2425 | 0.4370 | 90.11% | 0.5566 |
| 2 | 912 | 0.2589 | 0.4066 | 90.65% | 0.5086 |
| 3 | 1368 | 0.2877 | 0.4001 | 90.90% | 0.4973 |
| 4 | 1824 | 0.1971 | 0.4009 | 90.97% | 0.4834 |
| 5 | 2280 | 0.2428 | 0.4112 | 90.95% | 0.4749 |

### KlÃ­ÄovÃ© metriky:

âœ… **Final Validation Loss:** 0.4112
âœ… **Best Validation Loss:** 0.4001 (epoch 3)
âœ… **Final Training Loss:** 0.2428
âœ… **Mean Token Accuracy:** 90.95%
âœ… **Loss improvement:** 0.4370 â†’ 0.4001 (-8.4%)

---

## â±ï¸ ÄŒas a nÃ¡klady

| Metrika | Hodnota |
|---------|---------|
| **ÄŒas trÃ©ninku** | 52 minut 11 sekund |
| **Cena GPU** | $0.44/hodina |
| **CelkovÃ¡ cena** | ~$0.38 |
| **ZpracovÃ¡no tokenÅ¯** | 1,360,310 |
| **KrokÅ¯ celkem** | 2,280 |

---

## ğŸ¯ AnalÃ½za vÃ½sledkÅ¯

### âœ… Pozitiva:

1. **VÃ½bornÃ¡ accuracy** - Model sprÃ¡vnÄ› pÅ™edpovÃ­dÃ¡ **91% tokenÅ¯**
2. **StabilnÃ­ konvergence** - Loss klesÃ¡ bez overfittingu
3. **RychlÃ½ trÃ©nink** - Jen 52 minut dÃ­ky LoRA
4. **NÃ­zkÃ© nÃ¡klady** - MÃ©nÄ› neÅ¾ $0.40 za celÃ½ trÃ©nink
5. **DobrÃ¡ generalizace** - Validation loss nepÅ™erostl training loss

### ğŸ“Š Trendy:

- **Training loss:** KlesÃ¡ a stabilizuje se kolem 0.24
- **Validation loss:** NejlepÅ¡Ã­ v epoch 3 (0.4001), mÃ­rnÄ› roste v epoch 5
- **Accuracy:** StabilnÄ› vysokÃ¡ 90-91%
- **Entropy:** KlesÃ¡ (0.557 â†’ 0.475) = model je si jistÄ›jÅ¡Ã­

### ğŸ’¡ DoporuÄenÃ­:

- âœ… Model je dobÅ™e natrÃ©novanÃ½
- âš ï¸ MÃ­rnÃ© znÃ¡mky overfittingu v epoch 5 (validation loss roste)
- ğŸ’¡ Pro produkci pouÅ¾Ã­t checkpoint z epoch 3-4 (nejlepÅ¡Ã­ validation loss)

---

## ğŸ¤— PublikovanÃ½ model

**HuggingFace Hub:**
[TomasBo/Essention_oils-Mistral-7B-Instruct-v0.3-lora-adapter](https://huggingface.co/TomasBo/Essention_oils-Mistral-7B-Instruct-v0.3-lora-adapter)

**Velikost:**
- Model adapters: 1.24 GB
- Tokenizer: 587 KB
- Celkem: ~1.24 GB

**PouÅ¾itÃ­:**
```python
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer

model = AutoPeftModelForCausalLM.from_pretrained(
    "TomasBo/Essention_oils-Mistral-7B-Instruct-v0.3-lora-adapter"
)
tokenizer = AutoTokenizer.from_pretrained(
    "TomasBo/Essention_oils-Mistral-7B-Instruct-v0.3-lora-adapter"
)
```

---

## ğŸ§ª TestovÃ¡nÃ­

Model byl testovÃ¡n na 9 otÃ¡zkÃ¡ch o esenciÃ¡lnÃ­ch olejÃ­ch:

1. âœ… JakÃ© jsou ÃºÄinky oregana na tÄ›lo?
2. âœ… JakÃ© oleje bys doporuÄil na psychickou Ãºnavu a stres?
3. âœ… KterÃ© esenciÃ¡lnÃ­ oleje pomÃ¡hajÃ­ pÅ™i zaÅ¾Ã­vacÃ­ch obtÃ­Å¾Ã­ch?
4. âœ… PomÅ¯Å¾e levandule pÅ™i nespavosti?
5. âœ… Na co se pouÅ¾Ã­vÃ¡ mÃ¡ta peprnÃ¡?
6. âœ… K Äemu je u oleje uvedenÃ¡ jeho frekvence?
7. âœ… JakÃ© esenciÃ¡lnÃ­ oleje bys doporuÄil na bolesti kloubÅ¯ a svalÅ¯?
8. âœ… KterÃ½ olej je dobrÃ½ na trÃ¡venÃ­?
9. âœ… Jak bys vytvoÅ™il smÄ›s olejÅ¯ na podporu spÃ¡nku?

**VÃ½sledky testovÃ¡nÃ­:**
- Viz `results_after_finetuning.txt`
- PorovnÃ¡nÃ­ s base modelem: `results_before_finetuning.txt`

---

## ğŸ“ Soubory

```
2-Fine_tuning/
â”œâ”€â”€ script-model-finetunning.ipynb    # HlavnÃ­ training notebook
â”œâ”€â”€ test_base_model.py                # Test base modelu (pÅ™ed)
â”œâ”€â”€ test_finetuned_model.py           # Test fine-tuned modelu (po)
â”œâ”€â”€ results_before_finetuning.txt     # OdpovÄ›di pÅ™ed fine-tuningem
â”œâ”€â”€ results_after_finetuning.txt      # OdpovÄ›di po fine-tuningu
â”œâ”€â”€ TRAINING_RESULTS.md               # Tento soubor
â””â”€â”€ fleurdin-mistral-7b/              # LokÃ¡lnÃ­ LoRA adapters
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.safetensors
    â””â”€â”€ ...
```

---

## ğŸ“ ZÃ¡vÄ›r

Fine-tuning byl **ÃºspÄ›Å¡nÃ½**! ğŸ‰

Model Mistral-7B-Instruct-v0.3 byl ÃºspÄ›Å¡nÄ› adaptovÃ¡n na dataset esenciÃ¡lnÃ­ch olejÅ¯ pomocÃ­ LoRA metody. VÃ½slednÃ½ model dosahuje:

- âœ… **91% token accuracy**
- âœ… **StabilnÃ­ konvergence**
- âœ… **NÃ­zkÃ© nÃ¡klady** ($0.38)
- âœ… **RychlÃ½ trÃ©nink** (52 minut)
- âœ… **VeÅ™ejnÄ› dostupnÃ½** na HuggingFace

Model je pÅ™ipravenÃ½ k pouÅ¾itÃ­ pro:
- ğŸ’¬ Chatbot o esenciÃ¡lnÃ­ch olejÃ­ch
- ğŸ” Q&A systÃ©m o aromaterapii
- ğŸ“š VzdÄ›lÃ¡vacÃ­ nÃ¡stroj
- ğŸ›’ E-commerce asistent

---

**VytvoÅ™eno:** 31. Å™Ã­jna 2025
**Autor:** TomasBo
**Projekt:** Fleurdin AI
**GitHub:** [Global-Classes-CZE/AI-developer-3](https://github.com/Global-Classes-CZE/AI-developer-3)

ğŸŒ¿ **Pro zdravÃ­ a wellness s pomocÃ­ AI!**
