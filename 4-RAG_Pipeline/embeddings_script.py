"""
FLEURDIN AI - EMBEDDINGS SCRIPT
================================
VytvoÅ™Ã­ vector embeddings pro kaÅ¾dÃ½ chunk.
"""

import json
from pathlib import Path
from sentence_transformers import SentenceTransformer
from tqdm import tqdm


# Konfigurace
EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"  # Pro ÄeÅ¡tinu/slovenÅ¡tinu
BATCH_SIZE = 32  # Kolik chunkÅ¯ zpracovat najednou (rychlost)

# Cesty k souborÅ¯m
INPUT_FILE = Path("/Users/atlas/Projects/Fleurdin_AI/4-RAG_Pipeline/chunked_data.json")
OUTPUT_FILE = Path("/Users/atlas/Projects/Fleurdin_AI/4-RAG_Pipeline/chunked_data_with_embeddings.json")


print("="*70)
print("ğŸ§  FLEURDIN AI - VYTVÃÅ˜ENÃ EMBEDDINGÅ®")
print("="*70)
print(f"\nModel: {EMBEDDING_MODEL}")
print(f"Batch size: {BATCH_SIZE}")

def create_embeddings(chunks, model):
    """
    VytvoÅ™Ã­ embeddings pro vÅ¡echny chunky.
    
    Parametry:
    - chunks: seznam chunkÅ¯
    - model: SentenceTransformer model
    """
    print("\n" + "-"*70)
    print("ğŸ”„ VYTVÃÅ˜ÃM EMBEDDINGS")
    print("-"*70)
    print(f"Celkem chunkÅ¯: {len(chunks)}")

    # PÅ™iprav texty pro embedding
    texts = [chunk['text'] for chunk in chunks]

    # VytvoÅ™ embeddings (s progress barem)
    print("\nâ³ ZpracovÃ¡vÃ¡m chunky...")
    embeddings = model.encode(
        texts,
        batch_size=BATCH_SIZE,
        show_progress_bar=True,
        convert_to_numpy=True
    )

    # PÅ™idej embeddings k chunkÅ¯m
    print("\nâœ… PÅ™idÃ¡vÃ¡m embeddings k chunkÅ¯m...")
    for i, chunk in enumerate(tqdm(chunks, desc="PÅ™idÃ¡vÃ¡nÃ­ embeddingÅ¯")):
        chunk['embedding'] = embeddings[i].tolist()

    print(f"\nâœ… Hotovo! VytvoÅ™eno {len(chunks)} embeddingÅ¯")
    print(f"ğŸ“ Velikost embeddingy: {len(embeddings[0])} dimenzÃ­")

    return chunks

def main():
    """
    HlavnÃ­ funkce - naÄte chunky, vytvoÅ™Ã­ embeddings, uloÅ¾Ã­.
    """

    # 1. NaÄti chunked data
    print("\n" + "-"*70)
    print("ğŸ“‚ NAÄŒÃTÃM DATA")
    print("-"*70)

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f"âœ… NaÄteno {data['stats']['total_chunks']} chunkÅ¯")

    # 2. NaÄti embedding model
    print("\n" + "-"*70)
    print("ğŸ¤– NAÄŒÃTÃM AI MODEL")
    print("-"*70)
    print("(PrvnÃ­ spuÅ¡tÄ›nÃ­ stÃ¡hne model ~120 MB)")

    model = SentenceTransformer(EMBEDDING_MODEL)
    print("âœ… Model naÄten!")

    # 3. VytvoÅ™ embeddings
    chunks_with_embeddings = create_embeddings(data['chunks'], model)

    # 4. UloÅ¾ vÃ½sledky
    print("\n" + "="*70)
    print("ğŸ’¾ UKLÃDÃM VÃSLEDKY")
    print("="*70)

    output_data = {
        "chunks": chunks_with_embeddings,
        "stats": data['stats'],
        "embedding_model": EMBEDDING_MODEL,
        "embedding_dimensions": len(chunks_with_embeddings[0]['embedding'])
    }

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… HOTOVO!")
    print(f"ğŸ“‚ VÃ½stup: {OUTPUT_FILE}")
    print(f"\nğŸ“Š FINÃLNÃ STATISTIKY:")
    print(f"  â€¢ Celkem chunkÅ¯: {len(chunks_with_embeddings)}")
    print(f"  â€¢ Embedding model: {EMBEDDING_MODEL}")
    print(f"  â€¢ Embedding dimenze: {output_data['embedding_dimensions']}")
    print(f"  â€¢ Velikost souboru: ~{OUTPUT_FILE.stat().st_size / 1024 / 1024:.1f} MB")
    print("\n" + "="*70)
    print("\nğŸ¯ DalÅ¡Ã­ krok: NahrÃ¡t data do vector databÃ¡ze (Supabase/Chroma)")


# SpusÅ¥ program
if __name__ == "__main__":
    main()