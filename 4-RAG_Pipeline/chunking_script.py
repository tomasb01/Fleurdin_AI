"""
FLEURDIN AI - CHUNKING SCRIPT
==============================
Rozdƒõl√≠ dlouh√© texty na men≈°√≠ chunky (1,200 znak≈Ø).
"""

import json
from pathlib import Path
from typing import List, Dict


# Konfigurace chunkingu
CONFIG = {
    "small_chunk_max": 1500,      # Men≈°√≠ ne≈æ toto = ponechat cel√©
    "chunk_size": 1200,            # Velikost chunku
    "overlap": 200                 # P≈ôekryv mezi chunky (17%)
}

# Cesty k soubor≈Øm
INPUT_FILE = Path("/Users/atlas/Projects/Fleurdin_AI/4-RAG_Pipeline/parsed_data.json")
OUTPUT_FILE = Path("/Users/atlas/Projects/Fleurdin_AI/4-RAG_Pipeline/chunked_data.json")


print("="*70)
print("üß© FLEURDIN AI - CHUNKING ZAƒå√çN√Å")
print("="*70)
print(f"\nParametry:")
print(f"  ‚Ä¢ Mal√© texty (<{CONFIG['small_chunk_max']} znak≈Ø) = ponechat cel√©")
print(f"  ‚Ä¢ Velk√© texty = rozdƒõlit na {CONFIG['chunk_size']} znak≈Ø")
print(f"  ‚Ä¢ Overlap: {CONFIG['overlap']} znak≈Ø")

def split_into_chunks(text, text_id, text_name):
    """
    Rozdƒõl√≠ dlouh√Ω text na men≈°√≠ chunky s overlapem.
    
    Parametry:
    - text: text k rozdƒõlen√≠
    - text_id: ID p≈Øvodn√≠ho textu
    - text_name: n√°zev textu (pro metadata)
    """
    text_length = len(text)

    # Je text mal√Ω? ‚Üí Ponechat cel√Ω
    if text_length <= CONFIG['small_chunk_max']:
        return [{
            "id": f"{text_id}_full",
            "text": text,
            "part": 1,
            "total_parts": 1,
            "name": text_name,
            "chunk_size": text_length
        }]

    # Text je velk√Ω ‚Üí Rozdƒõlit na chunky
    chunks = []
    start = 0
    part = 1
    chunk_size = CONFIG['chunk_size']
    overlap = CONFIG['overlap']

    while start < text_length:
        # Vezmi kus textu
        end = start + chunk_size
        chunk_text = text[start:end]

        # Ulo≈æ chunk
        chunks.append({
            "id": f"{text_id}_part_{part}",
            "text": chunk_text,
            "part": part,
            "total_parts": 0,  # Vypoƒç√≠t√°me pozdƒõji
            "name": text_name,
            "chunk_size": len(chunk_text)
        })

        # Posu≈à se d√°l (s overlapem)
        start += (chunk_size - overlap)
        part += 1

    # Aktualizuj total_parts
    total = len(chunks)
    for chunk in chunks:
        chunk['total_parts'] = total

    return chunks

def process_all_data(input_file):
    """
    Naƒçte parsed_data.json a aplikuje chunking.
    """
    print("\n" + "-"*70)
    print("üìÇ NAƒå√çT√ÅM DATA")
    print("-"*70)

    # Naƒçti JSON
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f"  ‚úÖ Naƒçteno {data['stats']['total_items']} polo≈æek")

    all_chunks = []
    stats = {
        "oils_chunks": 0,
        "book1_chunks": 0,
        "book2_chunks": 0,
        "drienky_chunks": 0
    }

    # 1. ESENCI√ÅLN√ç OLEJE - ponechat cel√©
    print("\n" + "-"*70)
    print("üåø ZPRACOV√ÅV√ÅM: Esenci√°ln√≠ oleje")
    print("-"*70)

    for oil in data['essential_oils']:
        chunks = split_into_chunks(
            text=oil['text'],
            text_id=oil['id'],
            text_name=oil['name']
        )

        # P≈ôidej metadata
        for chunk in chunks:
            chunk['type'] = 'essential_oil'
            chunk['tier'] = 'free'  # m≈Ø≈æe≈° zmƒõnit
            chunk['metadata'] = oil['metadata']

        all_chunks.extend(chunks)
        stats['oils_chunks'] += len(chunks)

    print(f"  ‚úÖ Vytvo≈ôeno {stats['oils_chunks']} chunk≈Ø")

    # 2. KNIHA 1
    print("\n" + "-"*70)
    print("üìñ ZPRACOV√ÅV√ÅM: Kniha 1")
    print("-"*70)

    for para in data['book1']:
        chunks = split_into_chunks(
            text=para['text'],
            text_id=para['id'],
            text_name=f"Kniha 1 - odstavec {para['metadata']['paragraph_number']}"
        )

        for chunk in chunks:
            chunk['type'] = 'book_paragraph'
            chunk['tier'] = 'premium'
            chunk['metadata'] = para['metadata']

        all_chunks.extend(chunks)
        stats['book1_chunks'] += len(chunks)

    print(f"  ‚úÖ Vytvo≈ôeno {stats['book1_chunks']} chunk≈Ø")

    # 3. KNIHA 2
    print("\n" + "-"*70)
    print("üìñ ZPRACOV√ÅV√ÅM: Kniha 2")
    print("-"*70)

    for para in data['book2']:
        chunks = split_into_chunks(
            text=para['text'],
            text_id=para['id'],
            text_name=f"Kniha 2 - odstavec {para['metadata']['paragraph_number']}"
        )

        for chunk in chunks:
            chunk['type'] = 'book_paragraph'
            chunk['tier'] = 'premium'
            chunk['metadata'] = para['metadata']

        all_chunks.extend(chunks)
        stats['book2_chunks'] += len(chunks)

    print(f"  ‚úÖ Vytvo≈ôeno {stats['book2_chunks']} chunk≈Ø")

    # 4. DRIENKY
    print("\n" + "-"*70)
    print("üé§ ZPRACOV√ÅV√ÅM: DRIENKY (voice p≈ôepis)")
    print("-"*70)

    # Spojit v≈°echny odstavce DRIENKY do jednoho textu
    drienky_text = "\n\n".join([p['text'] for p in data['drienky']])

    chunks = split_into_chunks(
        text=drienky_text,
        text_id="drienky_voice",
        text_name="DRIENKY - voice p≈ôepis"
    )

    for chunk in chunks:
        chunk['type'] = 'voice_transcript'
        chunk['tier'] = 'premium'
        chunk['metadata'] = {'source': 'voice_transcript', 'topic': 'drienky'}

    all_chunks.extend(chunks)
    stats['drienky_chunks'] = len(chunks)

    print(f"  ‚úÖ Vytvo≈ôeno {stats['drienky_chunks']} chunk≈Ø")

    return all_chunks, stats

def main():
    """
    Hlavn√≠ funkce - spust√≠ chunking.
    """

    # Zpracuj v≈°echna data
    all_chunks, stats = process_all_data(INPUT_FILE)

    # P≈ôiprav v√Ωstupn√≠ data
    output_data = {
        "chunks": all_chunks,
        "stats": {
            "total_chunks": len(all_chunks),
            "oils_chunks": stats['oils_chunks'],
            "book1_chunks": stats['book1_chunks'],
            "book2_chunks": stats['book2_chunks'],
            "drienky_chunks": stats['drienky_chunks']
        }
    }

    # Ulo≈æ do JSON
    print("\n" + "="*70)
    print("üíæ UKL√ÅD√ÅM V√ùSLEDKY")
    print("="*70)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ HOTOVO!")
    print(f"üìÇ V√Ωstup: {OUTPUT_FILE}")
    print(f"\nüìä FIN√ÅLN√ç STATISTIKY:")
    print(f"  ‚Ä¢ Esenci√°ln√≠ oleje: {stats['oils_chunks']} chunk≈Ø")
    print(f"  ‚Ä¢ Kniha 1: {stats['book1_chunks']} chunk≈Ø")
    print(f"  ‚Ä¢ Kniha 2: {stats['book2_chunks']} chunk≈Ø")
    print(f"  ‚Ä¢ DRIENKY: {stats['drienky_chunks']} chunk≈Ø")
    print(f"  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    print(f"  ‚Ä¢ CELKEM: {len(all_chunks)} chunk≈Ø")
    print("\n" + "="*70)
    print("\nüéØ Dal≈°√≠ krok: Vytvo≈ôen√≠ embeddings (vektorizace)")


# Spus≈• program
if __name__ == "__main__":
    main()