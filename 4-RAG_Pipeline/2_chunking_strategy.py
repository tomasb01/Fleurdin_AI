"""
FLEURDIN AI - CHUNKING STRATEGY IMPLEMENTATION
==============================================
Implementace hybridn√≠ chunking strategie (entity-based + fixed-size)

Parametry (optimalizov√°no pro GPT-4-mini):
- Entity-based: <1,500 znak≈Ø = cel√©, <2,500 znak≈Ø = cel√©
- Fixed-size: 1,200 znak≈Ø, 200 overlap (17%)
- Embedding model: paraphrase-multilingual-MiniLM-L12-v2
"""

import json
from pathlib import Path
from typing import List, Dict, Tuple
import re


class ChunkingStrategy:
    """Implementace hybridn√≠ chunking strategie"""

    def __init__(self):
        # Chunking parametry (optimalizov√°no pro GPT-4-mini + n√°klady)
        self.config = {
            "entity_based": {
                "small_entity_max": 1500,      # Mal√© entity - ponechat cel√©
                "medium_entity_max": 2500      # St≈ôedn√≠ entity - ponechat cel√©
            },
            "fixed_size": {
                "chunk_size": 1200,            # KOMPROMIS pro n√°klady + kvalitu
                "overlap": 200                 # 17% overlap
            },
            "heading_detection": {
                "max_length": 100,             # Max d√©lka nadpisu
                "keywords": [
                    "Kapitola", "kapitola",
                    "√övod", "√∫vod",
                    "Preƒço", "Ako", "ƒåo",
                    "?", ":"
                ]
            }
        }

    def chunk_all_data(self, parsed_data_path: str) -> Dict:
        """Aplikuje chunking strategii na v≈°echna data"""

        print("\n" + "="*70)
        print("üß© CHUNKING STRATEGY - APLIKACE")
        print("="*70)
        print(f"\nParametry:")
        print(f"  ‚Ä¢ Mal√© entity: <{self.config['entity_based']['small_entity_max']} znak≈Ø")
        print(f"  ‚Ä¢ St≈ôedn√≠ entity: <{self.config['entity_based']['medium_entity_max']} znak≈Ø")
        print(f"  ‚Ä¢ Fixed-size: {self.config['fixed_size']['chunk_size']} znak≈Ø")
        print(f"  ‚Ä¢ Overlap: {self.config['fixed_size']['overlap']} znak≈Ø")

        # Load parsed data
        with open(parsed_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # V√Ωstupn√≠ struktura
        chunked_data = {
            "essential_oils": [],
            "herbs_books": [],
            "voice_transcripts": [],
            "stats": {}
        }

        # 1. ESENCI√ÅLN√ç OLEJE - ji≈æ jsou optim√°ln√≠ chunky
        print("\n" + "-"*70)
        print("1Ô∏è‚É£  ESENCI√ÅLN√ç OLEJE")
        oils = [c for c in data['chunks'] if c['type'] == 'essential_oil']
        chunked_data["essential_oils"] = self._chunk_essential_oils(oils)

        # 2. KNIHY O BYLINK√ÅCH - detekce entit + hybridn√≠ chunking
        print("\n" + "-"*70)
        print("2Ô∏è‚É£  KNIHY O BYLINK√ÅCH")
        book1_paras = [c for c in data['chunks'] if c['type'] == 'herb_book' and 'book1' in c['id']]
        book2_paras = [c for c in data['chunks'] if c['type'] == 'herb_book' and 'book2' in c['id']]

        chunked_data["herbs_books"].extend(self._chunk_book(book1_paras, "book1"))
        chunked_data["herbs_books"].extend(self._chunk_book(book2_paras, "book2"))

        # 3. VOICE TRANSKRIPTY - fixed-size chunking
        print("\n" + "-"*70)
        print("3Ô∏è‚É£  VOICE TRANSKRIPTY")
        transcripts = [c for c in data['chunks'] if c['type'] == 'transcript']
        chunked_data["voice_transcripts"] = self._chunk_voice_transcripts(transcripts)

        # Stats
        chunked_data["stats"] = {
            "essential_oils": len(chunked_data["essential_oils"]),
            "herbs_books": len(chunked_data["herbs_books"]),
            "voice_transcripts": len(chunked_data["voice_transcripts"]),
            "total": (
                len(chunked_data["essential_oils"]) +
                len(chunked_data["herbs_books"]) +
                len(chunked_data["voice_transcripts"])
            )
        }

        return chunked_data

    def _chunk_essential_oils(self, oils: List[Dict]) -> List[Dict]:
        """
        Esenci√°ln√≠ oleje - ponechat jako cel√© chunky
        Jsou ji≈æ optim√°ln√≠ (~1,000-1,500 znak≈Ø)
        """
        print(f"  ‚Ä¢ Zpracov√°v√°m {len(oils)} olej≈Ø...")

        chunked_oils = []
        for oil in oils:
            size = len(oil['text'])

            # P≈ôidat metadata pro RAG
            chunk = {
                "id": oil['id'],
                "type": "essential_oil",
                "entity_name": oil['name'],
                "entity_type": "essential_oil",
                "text": oil['text'],
                "part": 1,
                "total_parts": 1,
                "tier": "free",  # Default - uprav√≠≈° podle pot≈ôeby
                "metadata": {
                    **oil['metadata'],
                    "english_name": oil.get('english_name', ''),
                    "latin_name": oil.get('latin_name', ''),
                    "frequency": oil.get('frequency', ''),
                    "chunk_size": size
                }
            }
            chunked_oils.append(chunk)

        print(f"  ‚úÖ Vytvo≈ôeno: {len(chunked_oils)} chunk≈Ø")
        print(f"  üìè Pr≈Ømƒõrn√° velikost: {sum(len(c['text']) for c in chunked_oils)//len(chunked_oils)} znak≈Ø")

        return chunked_oils

    def _chunk_book(self, paragraphs: List[Dict], book_id: str) -> List[Dict]:
        """
        Knihy - detekce entit + hybridn√≠ chunking
        """
        print(f"\n  üìñ {book_id.upper()}")
        print(f"  ‚Ä¢ Zpracov√°v√°m {len(paragraphs)} odstavc≈Ø...")

        # 1. Detekce entit (kapitol, bylin)
        entities = self._detect_entities(paragraphs)
        print(f"  ‚Ä¢ Detekov√°no {len(entities)} entit")

        # 2. Chunking podle velikosti entity
        chunked_entities = []
        for entity in entities:
            entity_chunks = self._chunk_entity(entity, book_id)
            chunked_entities.extend(entity_chunks)

        print(f"  ‚úÖ Vytvo≈ôeno: {len(chunked_entities)} chunk≈Ø")

        # Stats
        small = len([c for c in chunked_entities if c['total_parts'] == 1])
        large = len([c for c in chunked_entities if c['total_parts'] > 1])
        print(f"  ‚Ä¢ Cel√© entity: {small}")
        print(f"  ‚Ä¢ Rozdƒõlen√© entity: {large}")

        return chunked_entities

    def _detect_entities(self, paragraphs: List[Dict]) -> List[Dict]:
        """
        Detekuje entity (kapitoly, bylinky) z odstavc≈Ø
        """
        entities = []
        current_entity = None

        for para in paragraphs:
            text = para['text']

            # Je to nadpis?
            if self._is_heading(text):
                # Ulo≈æit p≈ôedchoz√≠ entitu
                if current_entity:
                    entities.append(current_entity)

                # Zaƒç√≠t novou entitu
                current_entity = {
                    "name": text.strip(),
                    "paragraphs": [],
                    "source_id": para['id']
                }
            else:
                # P≈ôidat k aktu√°ln√≠ entitƒõ
                if current_entity:
                    current_entity["paragraphs"].append(para)
                else:
                    # Odstavec bez nadpisu - vytvo≈ôit vlastn√≠ entitu
                    entities.append({
                        "name": f"Odstavec {para['id']}",
                        "paragraphs": [para],
                        "source_id": para['id']
                    })

        # Ulo≈æit posledn√≠ entitu
        if current_entity:
            entities.append(current_entity)

        return entities

    def _is_heading(self, text: str) -> bool:
        """
        Detekuje, zda je text nadpis
        """
        # Kr√°tk√Ω text (<100 znak≈Ø)
        if len(text) > self.config['heading_detection']['max_length']:
            return False

        # Obsahuje kl√≠ƒçov√° slova
        keywords = self.config['heading_detection']['keywords']
        for keyword in keywords:
            if keyword.lower() in text.lower():
                return True

        # Zaƒç√≠n√° velk√Ωm p√≠smenem + obsahuje ƒç√≠slo
        if text[0].isupper() and any(char.isdigit() for char in text):
            return True

        # Cel√© uppercase (nap≈ô. "P√öPAVA LEK√ÅRSKA")
        if text.isupper() and len(text.split()) <= 5:
            return True

        return False

    def _chunk_entity(self, entity: Dict, book_id: str) -> List[Dict]:
        """
        Chunking jedn√© entity podle velikosti
        """
        # Spojit odstavce do souvisl√©ho textu
        full_text = "\n\n".join([p['text'] for p in entity['paragraphs']])
        entity_size = len(full_text)

        # Rozhodnut√≠ podle velikosti
        small_max = self.config['entity_based']['small_entity_max']
        medium_max = self.config['entity_based']['medium_entity_max']

        if entity_size <= small_max:
            # MAL√Å ENTITA - ponechat celou
            return self._create_single_chunk(entity, full_text, book_id)

        elif entity_size <= medium_max:
            # ST≈òEDN√ç ENTITA - ponechat celou (na hranici)
            return self._create_single_chunk(entity, full_text, book_id)

        else:
            # VELK√Å ENTITA - rozdƒõlit fixed-size
            return self._create_fixed_size_chunks(entity, full_text, book_id)

    def _create_single_chunk(self, entity: Dict, text: str, book_id: str) -> List[Dict]:
        """Vytvo≈ô√≠ jeden chunk z cel√© entity"""
        chunk = {
            "id": f"{book_id}_{entity['source_id']}_full",
            "type": "herb_knowledge",
            "entity_name": entity['name'],
            "entity_type": "herb",
            "text": f"{entity['name']}\n\n{text}",
            "part": 1,
            "total_parts": 1,
            "tier": "premium",  # Default - uprav√≠≈° podle pot≈ôeby
            "metadata": {
                "source": book_id,
                "category": "bylinky",
                "chunk_size": len(text)
            }
        }
        return [chunk]

    def _create_fixed_size_chunks(self, entity: Dict, text: str, book_id: str) -> List[Dict]:
        """Vytvo≈ô√≠ fixed-size chunky z velk√© entity"""
        chunk_size = self.config['fixed_size']['chunk_size']
        overlap = self.config['fixed_size']['overlap']

        chunks = []
        start = 0
        part = 1

        # P≈ôidat n√°zev entity na zaƒç√°tek prvn√≠ho chunku
        text_with_header = f"{entity['name']}\n\n{text}"

        while start < len(text_with_header):
            end = start + chunk_size
            chunk_text = text_with_header[start:end]

            # Vytvo≈ôit chunk
            chunk = {
                "id": f"{book_id}_{entity['source_id']}_part_{part}",
                "type": "herb_knowledge",
                "entity_name": entity['name'],
                "entity_type": "herb",
                "text": chunk_text,
                "part": part,
                "total_parts": 0,  # Vypoƒç√≠t√°me pozdƒõji
                "tier": "premium",
                "metadata": {
                    "source": book_id,
                    "category": "bylinky",
                    "chunk_size": len(chunk_text)
                }
            }
            chunks.append(chunk)

            start += (chunk_size - overlap)
            part += 1

        # Update total_parts
        total = len(chunks)
        for chunk in chunks:
            chunk['total_parts'] = total

        return chunks

    def _chunk_voice_transcripts(self, transcripts: List[Dict]) -> List[Dict]:
        """
        Voice transkripty - spojit vƒõty + fixed-size chunking
        """
        print(f"  ‚Ä¢ Zpracov√°v√°m {len(transcripts)} vƒõt...")

        # Spojit v≈°echny vƒõty do souvisl√©ho textu
        full_text = " ".join([t['text'] for t in transcripts])

        print(f"  ‚Ä¢ Celkov√° d√©lka: {len(full_text)} znak≈Ø")

        # Fixed-size chunking
        chunk_size = self.config['fixed_size']['chunk_size']
        overlap = self.config['fixed_size']['overlap']

        chunks = []
        start = 0
        part = 1

        while start < len(full_text):
            end = start + chunk_size
            chunk_text = full_text[start:end]

            chunk = {
                "id": f"voice_transcript_drienka_part_{part}",
                "type": "herb_knowledge",
                "entity_name": "Drienka obyƒçajn√°",  # Z n√°zvu transkriptu
                "entity_type": "herb",
                "text": chunk_text,
                "part": part,
                "total_parts": 0,  # Vypoƒç√≠t√°me pozdƒõji
                "tier": "premium",
                "metadata": {
                    "source": "voice_transcript",
                    "category": "bylinky",
                    "duration_minutes": 7.5,  # Z JSON
                    "chunk_size": len(chunk_text)
                }
            }
            chunks.append(chunk)

            start += (chunk_size - overlap)
            part += 1

        # Update total_parts
        total = len(chunks)
        for chunk in chunks:
            chunk['total_parts'] = total

        print(f"  ‚úÖ Vytvo≈ôeno: {len(chunks)} chunk≈Ø")
        print(f"  üìè Pr≈Ømƒõrn√° velikost: {sum(len(c['text']) for c in chunks)//len(chunks)} znak≈Ø")

        return chunks


def main():
    """Main function"""
    base_path = Path("/Users/atlas/Projects/Fleurdin_AI/4-RAG_Pipeline")

    # Paths
    parsed_data_path = base_path / "parsed_data.json"
    output_path = base_path / "chunked_data.json"

    # Initialize chunking strategy
    chunker = ChunkingStrategy()

    # Apply chunking
    chunked_data = chunker.chunk_all_data(str(parsed_data_path))

    # Save results
    print("\n" + "="*70)
    print("üíæ UKL√ÅD√ÅM V√ùSLEDKY")
    print("="*70)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunked_data, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ Ulo≈æeno do: {output_path}")
    print(f"\nüìä FIN√ÅLN√ç STATISTIKY:")
    print(f"  ‚Ä¢ Esenci√°ln√≠ oleje: {chunked_data['stats']['essential_oils']} chunk≈Ø")
    print(f"  ‚Ä¢ Knihy o bylink√°ch: {chunked_data['stats']['herbs_books']} chunk≈Ø")
    print(f"  ‚Ä¢ Voice transkripty: {chunked_data['stats']['voice_transcripts']} chunk≈Ø")
    print(f"  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    print(f"  ‚Ä¢ CELKEM: {chunked_data['stats']['total']} chunk≈Ø")

    print("\n" + "="*70)
    print("‚úÖ CHUNKING DOKONƒåEN!")
    print("="*70)
    print("\nüéØ Dal≈°√≠ krok: Vytvo≈ôen√≠ embeddings (sentence-transformers)")


if __name__ == "__main__":
    main()
