{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained on RunPod.io\n",
    "\n",
    "- GPU - RTX 3090 24GB / A5000 24 GB\n",
    "- RAM - 21 GB \n",
    "- HDD - 200 GB\n",
    "\n",
    "Price 0.50$/hod\n",
    "\n",
    "## 4-bit training\n",
    "\n",
    "- training took cca. 15 minutes = 0.11 $\n",
    "\n",
    "## 16-bit merged model\n",
    "\n",
    "- merge took cca. 2 minute = 0.02 $\n",
    "- push took cca. 2 minute = 0.02 $\n",
    "\n",
    "\n",
    "# Inference on TGI \n",
    "https://ui.endpoints.huggingface.co/\n",
    "\n",
    "GPU - L4 16GB VRAM\n",
    "\n",
    "Price 0.8$/hod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()):\n\u001b[1;32m      3\u001b[0m    \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_properties(i)\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 73.6 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch) (4.15.0)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 23.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading markupsafe-3.0.3-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: mpmath, MarkupSafe, sympy, networkx, jinja2, fsspec, filelock, torch\n",
      "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.10.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 sympy-1.14.0 torch-2.8.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 105.0 MB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.3 MB 25.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<2.3,>=1.22.4\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 23.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0 numpy-2.0.2 scipy-1.13.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "\u001b[K     |████████████████████████████████| 374 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "\u001b[K     |████████████████████████████████| 432 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading pyyaml-6.0.3-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate) (2.8.0)\n",
      "Collecting huggingface_hub>=0.21.0\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: psutil in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate) (25.0)\n",
      "Collecting shellingham\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: filelock in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.42.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer-slim\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 6.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting anyio\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11>=0.16\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: networkx in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sniffio, idna, h11, certifi, httpcore, click, anyio, typer-slim, tqdm, shellingham, pyyaml, httpx, hf-xet, safetensors, huggingface-hub, accelerate\n",
      "Successfully installed accelerate-1.10.1 anyio-4.11.0 certifi-2025.10.5 click-8.1.8 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.0.1 idna-3.11 pyyaml-6.0.3 safetensors-0.6.2 shellingham-1.5.4 sniffio-1.3.1 tqdm-4.67.1 typer-slim-0.20.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 4.4 MB/s eta 0:00:01     |██████████████████████████████▌ | 11.4 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 7.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.4.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: filelock in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers) (3.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[K     |████████████████████████████████| 566 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2025.10.23-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.4-cp39-cp39-macosx_10_9_universal2.whl (209 kB)\n",
      "\u001b[K     |████████████████████████████████| 209 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.11)\n",
      "Installing collected packages: urllib3, charset-normalizer, requests, huggingface-hub, tokenizers, regex, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 1.0.1\n",
      "    Uninstalling huggingface-hub-1.0.1:\n",
      "      Successfully uninstalled huggingface-hub-1.0.1\n",
      "Successfully installed charset-normalizer-3.4.4 huggingface-hub-0.36.0 regex-2025.10.23 requests-2.32.5 tokenizers-0.22.1 transformers-4.57.1 urllib3-2.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "\u001b[K     |████████████████████████████████| 504 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: safetensors in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: transformers in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (4.57.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: tqdm in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: requests in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: filelock in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\n",
      "Requirement already satisfied: jinja2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: networkx in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers->peft) (2025.10.23)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers->peft) (0.22.1)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.17.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[K     |████████████████████████████████| 506 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.5)\n",
      "Collecting fsspec[http]<=2025.9.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: httpx<1.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets) (0.28.1)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp39-cp39-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.8 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.4.1,>=0.3.0\n",
      "  Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.3)\n",
      "Collecting pyarrow>=21.0.0\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.6.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Requirement already satisfied: filelock in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets) (2.0.2)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.13.2-cp39-cp39-macosx_11_0_arm64.whl (490 kB)\n",
      "\u001b[K     |████████████████████████████████| 490 kB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.8.0-cp39-cp39-macosx_11_0_arm64.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=17.3.0\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 25.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.22.0-cp39-cp39-macosx_11_0_arm64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.7.0-cp39-cp39-macosx_11_0_arm64.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.4.1-cp39-cp39-macosx_11_0_arm64.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 31.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: idna in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: anyio in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 57.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: propcache, multidict, frozenlist, yarl, attrs, async-timeout, aiosignal, aiohappyeyeballs, tzdata, pytz, fsspec, dill, aiohttp, xxhash, pyarrow, pandas, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.10.0\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 async-timeout-5.0.1 attrs-25.4.0 datasets-4.3.0 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.9.0 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 propcache-0.4.1 pyarrow-21.0.0 pytz-2025.2 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 585 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (2025.9.0)\n",
      "Requirement already satisfied: pandas in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: multiprocess in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: packaging in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (4.3.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: filelock in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: certifi in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: anyio in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: idna in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.15.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting trl\n",
      "  Downloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
      "\u001b[K     |████████████████████████████████| 423 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=1.4.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from trl) (1.10.1)\n",
      "Requirement already satisfied: transformers!=4.57.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from trl) (4.57.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from trl) (4.3.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate>=1.4.0->trl) (2.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: pyyaml in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
      "Requirement already satisfied: psutil in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate>=1.4.0->trl) (7.1.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: xxhash in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: pandas in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: fsspec[http]<=2025.9.0,>=2023.1.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (2025.9.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: filelock in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (4.15.0)\n",
      "Requirement already satisfied: idna in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.10.5)\n",
      "Requirement already satisfied: anyio in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.11.0)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: networkx in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers!=4.57.0->trl) (2025.10.23)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from transformers!=4.57.0->trl) (0.22.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.15.0)\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.24.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 975 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 941 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "\u001b[K     |████████████████████████████████| 249 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.0.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.1-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.3.0-cp39-cp39-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 988 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.60.1 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pillow-11.3.0 pyparsing-3.2.5\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 903 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.76.0-cp39-cp39-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[K     |████████████████████████████████| 135 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from tensorboard) (2.0.2)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard) (58.0.4)\n",
      "Requirement already satisfied: packaging in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: pillow in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from tensorboard) (11.3.0)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "\u001b[K     |████████████████████████████████| 427 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=4.12 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/atlas/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.3.1 grpcio-1.76.0 markdown-3.9 protobuf-6.33.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp39-cp39-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: hf-transfer\n",
      "Successfully installed hf-transfer-0.1.9\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "%pip install transformers\n",
    "%pip install peft\n",
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "%pip install trl\n",
    "%pip install matplotlib\n",
    "%pip install tensorboard\n",
    "%pip install sentencepiece\n",
    "%pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atlas/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/atlas/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "API_TOKEN = \"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\"\n",
    "login(token=API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174c9a4b13954f69ae085240b8f17d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(32768, 4096)\n",
      "Linear(in_features=4096, out_features=32768, bias=False)\n",
      "Model Vocabulary Size: 32768\n",
      "before 32768\n",
      "after 32769\n",
      "before 2 None\n",
      "after 2 32768\n",
      "Model Vocabulary Size: 32768\n",
      "Model Vocabulary Size: 32769\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(base_model.get_input_embeddings())\n",
    "print(base_model.get_output_embeddings())\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"before\", len(base_tokenizer))\n",
    "base_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "print(\"after\", len(base_tokenizer))\n",
    "\n",
    "print(\"before 2\", base_model.config.pad_token_id)\n",
    "base_model.config.pad_token_id = base_tokenizer.pad_token_id\n",
    "base_model.generation_config.pad_token_id = base_tokenizer.pad_token_id\n",
    "print(\"after 2\", base_model.config.pad_token_id)\n",
    "\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "base_model.resize_token_embeddings(len(base_tokenizer))\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Tell me a joke about programmers.\n",
      "Response: Why did the programmer write a message on the bathroom wall in the office? He wanted to \"make a clean break\" from the old code. (A play on words, as in \"clean\" as in a clean break, but also \"clean\" as in writing on a bathroom wall.)\n",
      "\n",
      "Prompt: What is Autogen?\n",
      "Response: Autogen, short for Autogenous training, is a type of machine learning technique used in reinforcement learning. In this method, the agent learns a policy directly from demonstrations, without a reward signal. The demonstrations are typically provided by a human expert or another agent.\n",
      "\n",
      "In other words, Autogen training involves using a model that has been pre-trained on a dataset to generate its own data. This self-generated data is then used to further train the model. This technique can be useful when the reward signal is sparse or difficult to define, as is often the case in complex, real-world problems.\n",
      "\n",
      "Autogen is a popular method in video game AI, where it is used to train agents to play games without explicit reward signals. For example, an agent can be trained to play a game like Atari by playing the game itself and learning from its own mistakes. This approach has been used to create AI agents that can play a variety of Atari games at a level comparable to human performance.\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, tokenizer, prompt):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt as a conversation\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens (skip the input)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test 1 - simple prompt\n",
    "prompt = \"Tell me a joke about programmers.\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(base_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Test 2 - Autogen related prompt\n",
    "prompt = \"What is Autogen?\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(base_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n",
      "Architecture: MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32769, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Generation Config: GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 32769\n",
      "Input embeddings:\n",
      "Embedding(32769, 4096)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=32769, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "All tokens count: 32769\n",
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(base_model))\n",
    "print(\"Architecture:\", base_model)\n",
    "print(\"Config:\", base_model.config)\n",
    "print(\"Generation Config:\", base_model.generation_config)\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(base_model.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(base_model.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(base_tokenizer))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", base_tokenizer.special_tokens_map)\n",
    "print(\"All tokens count:\", len(base_tokenizer))\n",
    "print(\"Padding side:\", base_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 222\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 222\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"lukaskellerstein/autogen\", split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.rename_column(\"text\", \"messages\") \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 177\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 45\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# final dataset\n",
    "final_datasets = dataset.train_test_split(test_size=0.2)\n",
    "print(final_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ----------------------------------\n",
    "# Adding the adapters to the layers\n",
    "# ----------------------------------\n",
    "\n",
    "# PEFT\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    # target_modules=[\n",
    "    #     \"q_proj\",\n",
    "    #     \"k_proj\",\n",
    "    #     \"down_proj\",\n",
    "    #     \"v_proj\",\n",
    "    #     \"gate_proj\",\n",
    "    #     \"o_proj\",\n",
    "    #     \"up_proj\",\n",
    "    # ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # modules_to_save=[\n",
    "    #     \"lm_head\",\n",
    "    #     \"embed_tokens\",\n",
    "    # ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",  # https://huggingface.co/docs/peft/en/developer_guides/lora#qlora-style-training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run: run_20251027_000226\n",
      "Output directory: /workspace/runs/run_20251027_000226/model\n",
      "Logging directory: /workspace/runs/run_20251027_000226/logs\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Create timestamped run directory for this training session\n",
    "run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"run_{run_timestamp}\"\n",
    "output_dir = f\"/workspace/runs/{run_name}/model\"\n",
    "logging_dir = f\"/workspace/runs/{run_name}/logs\"\n",
    "\n",
    "print(f\"Training run: {run_name}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Logging directory: {logging_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ----------------------------------\n",
    "# Training WITH evaluation (metrics)\n",
    "# ----------------------------------\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "bs = 1  # batch size\n",
    "ga_steps = 4  # gradient acc. steps\n",
    "epochs = 5\n",
    "steps_per_epoch = len(final_datasets[\"train\"]) // (bs * ga_steps)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    learning_rate=lr,\n",
    "    save_steps=steps_per_epoch,\n",
    "    save_total_limit=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=steps_per_epoch,  # eval and save once per epoch\n",
    "    logging_steps=10,\n",
    "    logging_dir=logging_dir,\n",
    "    report_to=\"tensorboard\",  # Enable TensorBoard logging\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=10,  # Gradual warmup\n",
    "    fp16=True,\n",
    "    # bf16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    processing_class=base_tokenizer,\n",
    "    train_dataset=final_datasets[\"train\"],\n",
    "    eval_dataset=final_datasets[\"test\"],\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log TRAINER - Model, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Trainer model ---\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32769, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Generation Config: GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"pad_token_id\": 32768\n",
      "}\n",
      "\n",
      "Get Trainable Parameters\n",
      "trainable params: 41,943,040 || all params: 7,289,974,784 || trainable%: 0.5754\n",
      "None\n",
      "--- Trainer tokenizer ---\n",
      "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.3', vocab_size=32768, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[/INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"[TOOL_CALLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"[AVAILABLE_TOOLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"[/AVAILABLE_TOOLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"[TOOL_RESULTS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"[/TOOL_RESULTS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"[control_8]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"[control_9]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"[control_10]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"[control_11]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t14: AddedToken(\"[control_12]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15: AddedToken(\"[control_13]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t16: AddedToken(\"[control_14]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t17: AddedToken(\"[control_15]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t18: AddedToken(\"[control_16]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t19: AddedToken(\"[control_17]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t20: AddedToken(\"[control_18]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t21: AddedToken(\"[control_19]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t22: AddedToken(\"[control_20]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t23: AddedToken(\"[control_21]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t24: AddedToken(\"[control_22]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t25: AddedToken(\"[control_23]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t26: AddedToken(\"[control_24]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t27: AddedToken(\"[control_25]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t28: AddedToken(\"[control_26]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t29: AddedToken(\"[control_27]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t30: AddedToken(\"[control_28]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t31: AddedToken(\"[control_29]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32: AddedToken(\"[control_30]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t33: AddedToken(\"[control_31]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t34: AddedToken(\"[control_32]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t35: AddedToken(\"[control_33]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t36: AddedToken(\"[control_34]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t37: AddedToken(\"[control_35]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t38: AddedToken(\"[control_36]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t39: AddedToken(\"[control_37]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t40: AddedToken(\"[control_38]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t41: AddedToken(\"[control_39]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t42: AddedToken(\"[control_40]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t43: AddedToken(\"[control_41]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t44: AddedToken(\"[control_42]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t45: AddedToken(\"[control_43]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t46: AddedToken(\"[control_44]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t47: AddedToken(\"[control_45]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t48: AddedToken(\"[control_46]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t49: AddedToken(\"[control_47]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50: AddedToken(\"[control_48]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t51: AddedToken(\"[control_49]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t52: AddedToken(\"[control_50]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t53: AddedToken(\"[control_51]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t54: AddedToken(\"[control_52]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t55: AddedToken(\"[control_53]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t56: AddedToken(\"[control_54]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t57: AddedToken(\"[control_55]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t58: AddedToken(\"[control_56]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t59: AddedToken(\"[control_57]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t60: AddedToken(\"[control_58]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t61: AddedToken(\"[control_59]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t62: AddedToken(\"[control_60]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t63: AddedToken(\"[control_61]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64: AddedToken(\"[control_62]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t65: AddedToken(\"[control_63]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t66: AddedToken(\"[control_64]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t67: AddedToken(\"[control_65]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t68: AddedToken(\"[control_66]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t69: AddedToken(\"[control_67]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t70: AddedToken(\"[control_68]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t71: AddedToken(\"[control_69]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t72: AddedToken(\"[control_70]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t73: AddedToken(\"[control_71]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t74: AddedToken(\"[control_72]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t75: AddedToken(\"[control_73]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t76: AddedToken(\"[control_74]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t77: AddedToken(\"[control_75]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t78: AddedToken(\"[control_76]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t79: AddedToken(\"[control_77]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t80: AddedToken(\"[control_78]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t81: AddedToken(\"[control_79]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t82: AddedToken(\"[control_80]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t83: AddedToken(\"[control_81]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t84: AddedToken(\"[control_82]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t85: AddedToken(\"[control_83]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t86: AddedToken(\"[control_84]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t87: AddedToken(\"[control_85]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t88: AddedToken(\"[control_86]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t89: AddedToken(\"[control_87]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t90: AddedToken(\"[control_88]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t91: AddedToken(\"[control_89]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92: AddedToken(\"[control_90]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t93: AddedToken(\"[control_91]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t94: AddedToken(\"[control_92]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t95: AddedToken(\"[control_93]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t96: AddedToken(\"[control_94]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t97: AddedToken(\"[control_95]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t98: AddedToken(\"[control_96]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t99: AddedToken(\"[control_97]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[control_98]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[control_99]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[control_100]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[control_101]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t104: AddedToken(\"[control_102]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t105: AddedToken(\"[control_103]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t106: AddedToken(\"[control_104]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t107: AddedToken(\"[control_105]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t108: AddedToken(\"[control_106]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t109: AddedToken(\"[control_107]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t110: AddedToken(\"[control_108]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t111: AddedToken(\"[control_109]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t112: AddedToken(\"[control_110]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t113: AddedToken(\"[control_111]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t114: AddedToken(\"[control_112]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t115: AddedToken(\"[control_113]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t116: AddedToken(\"[control_114]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t117: AddedToken(\"[control_115]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t118: AddedToken(\"[control_116]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t119: AddedToken(\"[control_117]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t120: AddedToken(\"[control_118]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t121: AddedToken(\"[control_119]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t122: AddedToken(\"[control_120]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t123: AddedToken(\"[control_121]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t124: AddedToken(\"[control_122]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t125: AddedToken(\"[control_123]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t126: AddedToken(\"[control_124]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t127: AddedToken(\"[control_125]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128: AddedToken(\"[control_126]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t129: AddedToken(\"[control_127]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130: AddedToken(\"[control_128]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t131: AddedToken(\"[control_129]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t132: AddedToken(\"[control_130]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t133: AddedToken(\"[control_131]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t134: AddedToken(\"[control_132]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t135: AddedToken(\"[control_133]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t136: AddedToken(\"[control_134]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t137: AddedToken(\"[control_135]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t138: AddedToken(\"[control_136]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t139: AddedToken(\"[control_137]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t140: AddedToken(\"[control_138]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t141: AddedToken(\"[control_139]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t142: AddedToken(\"[control_140]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t143: AddedToken(\"[control_141]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t144: AddedToken(\"[control_142]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t145: AddedToken(\"[control_143]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t146: AddedToken(\"[control_144]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t147: AddedToken(\"[control_145]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t148: AddedToken(\"[control_146]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t149: AddedToken(\"[control_147]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t150: AddedToken(\"[control_148]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151: AddedToken(\"[control_149]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t152: AddedToken(\"[control_150]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t153: AddedToken(\"[control_151]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t154: AddedToken(\"[control_152]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t155: AddedToken(\"[control_153]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t156: AddedToken(\"[control_154]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t157: AddedToken(\"[control_155]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t158: AddedToken(\"[control_156]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t159: AddedToken(\"[control_157]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t160: AddedToken(\"[control_158]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t161: AddedToken(\"[control_159]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t162: AddedToken(\"[control_160]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t163: AddedToken(\"[control_161]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t164: AddedToken(\"[control_162]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t165: AddedToken(\"[control_163]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t166: AddedToken(\"[control_164]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t167: AddedToken(\"[control_165]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t168: AddedToken(\"[control_166]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t169: AddedToken(\"[control_167]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t170: AddedToken(\"[control_168]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t171: AddedToken(\"[control_169]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t172: AddedToken(\"[control_170]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t173: AddedToken(\"[control_171]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t174: AddedToken(\"[control_172]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t175: AddedToken(\"[control_173]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t176: AddedToken(\"[control_174]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t177: AddedToken(\"[control_175]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t178: AddedToken(\"[control_176]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t179: AddedToken(\"[control_177]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t180: AddedToken(\"[control_178]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t181: AddedToken(\"[control_179]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t182: AddedToken(\"[control_180]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t183: AddedToken(\"[control_181]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t184: AddedToken(\"[control_182]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t185: AddedToken(\"[control_183]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t186: AddedToken(\"[control_184]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t187: AddedToken(\"[control_185]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t188: AddedToken(\"[control_186]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t189: AddedToken(\"[control_187]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t190: AddedToken(\"[control_188]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t191: AddedToken(\"[control_189]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t192: AddedToken(\"[control_190]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t193: AddedToken(\"[control_191]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t194: AddedToken(\"[control_192]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t195: AddedToken(\"[control_193]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t196: AddedToken(\"[control_194]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t197: AddedToken(\"[control_195]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t198: AddedToken(\"[control_196]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t199: AddedToken(\"[control_197]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t200: AddedToken(\"[control_198]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t201: AddedToken(\"[control_199]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t202: AddedToken(\"[control_200]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t203: AddedToken(\"[control_201]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t204: AddedToken(\"[control_202]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t205: AddedToken(\"[control_203]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t206: AddedToken(\"[control_204]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t207: AddedToken(\"[control_205]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t208: AddedToken(\"[control_206]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t209: AddedToken(\"[control_207]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t210: AddedToken(\"[control_208]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t211: AddedToken(\"[control_209]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t212: AddedToken(\"[control_210]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t213: AddedToken(\"[control_211]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t214: AddedToken(\"[control_212]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t215: AddedToken(\"[control_213]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t216: AddedToken(\"[control_214]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t217: AddedToken(\"[control_215]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t218: AddedToken(\"[control_216]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t219: AddedToken(\"[control_217]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t220: AddedToken(\"[control_218]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t221: AddedToken(\"[control_219]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t222: AddedToken(\"[control_220]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t223: AddedToken(\"[control_221]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t224: AddedToken(\"[control_222]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t225: AddedToken(\"[control_223]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t226: AddedToken(\"[control_224]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t227: AddedToken(\"[control_225]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t228: AddedToken(\"[control_226]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t229: AddedToken(\"[control_227]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t230: AddedToken(\"[control_228]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t231: AddedToken(\"[control_229]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t232: AddedToken(\"[control_230]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t233: AddedToken(\"[control_231]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t234: AddedToken(\"[control_232]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t235: AddedToken(\"[control_233]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t236: AddedToken(\"[control_234]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t237: AddedToken(\"[control_235]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t238: AddedToken(\"[control_236]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t239: AddedToken(\"[control_237]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t240: AddedToken(\"[control_238]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t241: AddedToken(\"[control_239]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t242: AddedToken(\"[control_240]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t243: AddedToken(\"[control_241]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t244: AddedToken(\"[control_242]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t245: AddedToken(\"[control_243]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t246: AddedToken(\"[control_244]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t247: AddedToken(\"[control_245]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t248: AddedToken(\"[control_246]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t249: AddedToken(\"[control_247]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250: AddedToken(\"[control_248]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t251: AddedToken(\"[control_249]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t252: AddedToken(\"[control_250]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t253: AddedToken(\"[control_251]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t254: AddedToken(\"[control_252]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t255: AddedToken(\"[control_253]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256: AddedToken(\"[control_254]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t257: AddedToken(\"[control_255]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t258: AddedToken(\"[control_256]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t259: AddedToken(\"[control_257]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t260: AddedToken(\"[control_258]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t261: AddedToken(\"[control_259]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t262: AddedToken(\"[control_260]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t263: AddedToken(\"[control_261]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t264: AddedToken(\"[control_262]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t265: AddedToken(\"[control_263]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t266: AddedToken(\"[control_264]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t267: AddedToken(\"[control_265]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t268: AddedToken(\"[control_266]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t269: AddedToken(\"[control_267]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t270: AddedToken(\"[control_268]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t271: AddedToken(\"[control_269]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t272: AddedToken(\"[control_270]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t273: AddedToken(\"[control_271]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t274: AddedToken(\"[control_272]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t275: AddedToken(\"[control_273]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t276: AddedToken(\"[control_274]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t277: AddedToken(\"[control_275]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t278: AddedToken(\"[control_276]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t279: AddedToken(\"[control_277]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t280: AddedToken(\"[control_278]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t281: AddedToken(\"[control_279]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t282: AddedToken(\"[control_280]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t283: AddedToken(\"[control_281]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t284: AddedToken(\"[control_282]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t285: AddedToken(\"[control_283]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t286: AddedToken(\"[control_284]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t287: AddedToken(\"[control_285]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t288: AddedToken(\"[control_286]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t289: AddedToken(\"[control_287]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t290: AddedToken(\"[control_288]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t291: AddedToken(\"[control_289]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t292: AddedToken(\"[control_290]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t293: AddedToken(\"[control_291]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t294: AddedToken(\"[control_292]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t295: AddedToken(\"[control_293]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t296: AddedToken(\"[control_294]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t297: AddedToken(\"[control_295]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t298: AddedToken(\"[control_296]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t299: AddedToken(\"[control_297]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t300: AddedToken(\"[control_298]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t301: AddedToken(\"[control_299]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t302: AddedToken(\"[control_300]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t303: AddedToken(\"[control_301]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t304: AddedToken(\"[control_302]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t305: AddedToken(\"[control_303]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t306: AddedToken(\"[control_304]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t307: AddedToken(\"[control_305]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t308: AddedToken(\"[control_306]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t309: AddedToken(\"[control_307]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t310: AddedToken(\"[control_308]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t311: AddedToken(\"[control_309]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t312: AddedToken(\"[control_310]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t313: AddedToken(\"[control_311]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t314: AddedToken(\"[control_312]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t315: AddedToken(\"[control_313]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t316: AddedToken(\"[control_314]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t317: AddedToken(\"[control_315]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t318: AddedToken(\"[control_316]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t319: AddedToken(\"[control_317]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t320: AddedToken(\"[control_318]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t321: AddedToken(\"[control_319]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t322: AddedToken(\"[control_320]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t323: AddedToken(\"[control_321]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t324: AddedToken(\"[control_322]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t325: AddedToken(\"[control_323]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t326: AddedToken(\"[control_324]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t327: AddedToken(\"[control_325]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t328: AddedToken(\"[control_326]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t329: AddedToken(\"[control_327]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t330: AddedToken(\"[control_328]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t331: AddedToken(\"[control_329]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t332: AddedToken(\"[control_330]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t333: AddedToken(\"[control_331]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t334: AddedToken(\"[control_332]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t335: AddedToken(\"[control_333]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t336: AddedToken(\"[control_334]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t337: AddedToken(\"[control_335]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t338: AddedToken(\"[control_336]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t339: AddedToken(\"[control_337]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t340: AddedToken(\"[control_338]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t341: AddedToken(\"[control_339]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t342: AddedToken(\"[control_340]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t343: AddedToken(\"[control_341]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t344: AddedToken(\"[control_342]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t345: AddedToken(\"[control_343]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t346: AddedToken(\"[control_344]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t347: AddedToken(\"[control_345]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t348: AddedToken(\"[control_346]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t349: AddedToken(\"[control_347]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t350: AddedToken(\"[control_348]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t351: AddedToken(\"[control_349]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t352: AddedToken(\"[control_350]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t353: AddedToken(\"[control_351]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t354: AddedToken(\"[control_352]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t355: AddedToken(\"[control_353]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t356: AddedToken(\"[control_354]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t357: AddedToken(\"[control_355]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t358: AddedToken(\"[control_356]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t359: AddedToken(\"[control_357]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t360: AddedToken(\"[control_358]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t361: AddedToken(\"[control_359]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t362: AddedToken(\"[control_360]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t363: AddedToken(\"[control_361]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t364: AddedToken(\"[control_362]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t365: AddedToken(\"[control_363]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t366: AddedToken(\"[control_364]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t367: AddedToken(\"[control_365]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t368: AddedToken(\"[control_366]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t369: AddedToken(\"[control_367]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t370: AddedToken(\"[control_368]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t371: AddedToken(\"[control_369]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t372: AddedToken(\"[control_370]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t373: AddedToken(\"[control_371]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t374: AddedToken(\"[control_372]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t375: AddedToken(\"[control_373]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t376: AddedToken(\"[control_374]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t377: AddedToken(\"[control_375]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t378: AddedToken(\"[control_376]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t379: AddedToken(\"[control_377]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t380: AddedToken(\"[control_378]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t381: AddedToken(\"[control_379]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t382: AddedToken(\"[control_380]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t383: AddedToken(\"[control_381]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t384: AddedToken(\"[control_382]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t385: AddedToken(\"[control_383]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t386: AddedToken(\"[control_384]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t387: AddedToken(\"[control_385]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t388: AddedToken(\"[control_386]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t389: AddedToken(\"[control_387]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t390: AddedToken(\"[control_388]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t391: AddedToken(\"[control_389]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t392: AddedToken(\"[control_390]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t393: AddedToken(\"[control_391]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t394: AddedToken(\"[control_392]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t395: AddedToken(\"[control_393]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t396: AddedToken(\"[control_394]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t397: AddedToken(\"[control_395]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t398: AddedToken(\"[control_396]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t399: AddedToken(\"[control_397]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t400: AddedToken(\"[control_398]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t401: AddedToken(\"[control_399]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t402: AddedToken(\"[control_400]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t403: AddedToken(\"[control_401]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t404: AddedToken(\"[control_402]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t405: AddedToken(\"[control_403]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t406: AddedToken(\"[control_404]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t407: AddedToken(\"[control_405]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t408: AddedToken(\"[control_406]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t409: AddedToken(\"[control_407]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t410: AddedToken(\"[control_408]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t411: AddedToken(\"[control_409]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t412: AddedToken(\"[control_410]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t413: AddedToken(\"[control_411]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t414: AddedToken(\"[control_412]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t415: AddedToken(\"[control_413]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t416: AddedToken(\"[control_414]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t417: AddedToken(\"[control_415]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t418: AddedToken(\"[control_416]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t419: AddedToken(\"[control_417]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t420: AddedToken(\"[control_418]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t421: AddedToken(\"[control_419]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t422: AddedToken(\"[control_420]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t423: AddedToken(\"[control_421]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t424: AddedToken(\"[control_422]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t425: AddedToken(\"[control_423]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t426: AddedToken(\"[control_424]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t427: AddedToken(\"[control_425]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t428: AddedToken(\"[control_426]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t429: AddedToken(\"[control_427]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t430: AddedToken(\"[control_428]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t431: AddedToken(\"[control_429]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t432: AddedToken(\"[control_430]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t433: AddedToken(\"[control_431]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t434: AddedToken(\"[control_432]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t435: AddedToken(\"[control_433]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t436: AddedToken(\"[control_434]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t437: AddedToken(\"[control_435]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t438: AddedToken(\"[control_436]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t439: AddedToken(\"[control_437]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t440: AddedToken(\"[control_438]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t441: AddedToken(\"[control_439]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t442: AddedToken(\"[control_440]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t443: AddedToken(\"[control_441]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t444: AddedToken(\"[control_442]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t445: AddedToken(\"[control_443]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t446: AddedToken(\"[control_444]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t447: AddedToken(\"[control_445]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t448: AddedToken(\"[control_446]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t449: AddedToken(\"[control_447]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t450: AddedToken(\"[control_448]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t451: AddedToken(\"[control_449]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t452: AddedToken(\"[control_450]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t453: AddedToken(\"[control_451]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t454: AddedToken(\"[control_452]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t455: AddedToken(\"[control_453]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t456: AddedToken(\"[control_454]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t457: AddedToken(\"[control_455]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t458: AddedToken(\"[control_456]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t459: AddedToken(\"[control_457]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t460: AddedToken(\"[control_458]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t461: AddedToken(\"[control_459]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t462: AddedToken(\"[control_460]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t463: AddedToken(\"[control_461]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t464: AddedToken(\"[control_462]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t465: AddedToken(\"[control_463]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t466: AddedToken(\"[control_464]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t467: AddedToken(\"[control_465]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t468: AddedToken(\"[control_466]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t469: AddedToken(\"[control_467]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t470: AddedToken(\"[control_468]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t471: AddedToken(\"[control_469]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t472: AddedToken(\"[control_470]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t473: AddedToken(\"[control_471]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t474: AddedToken(\"[control_472]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t475: AddedToken(\"[control_473]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t476: AddedToken(\"[control_474]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t477: AddedToken(\"[control_475]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t478: AddedToken(\"[control_476]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t479: AddedToken(\"[control_477]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t480: AddedToken(\"[control_478]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t481: AddedToken(\"[control_479]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t482: AddedToken(\"[control_480]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t483: AddedToken(\"[control_481]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t484: AddedToken(\"[control_482]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t485: AddedToken(\"[control_483]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t486: AddedToken(\"[control_484]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t487: AddedToken(\"[control_485]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t488: AddedToken(\"[control_486]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t489: AddedToken(\"[control_487]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t490: AddedToken(\"[control_488]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t491: AddedToken(\"[control_489]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t492: AddedToken(\"[control_490]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t493: AddedToken(\"[control_491]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t494: AddedToken(\"[control_492]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t495: AddedToken(\"[control_493]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t496: AddedToken(\"[control_494]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t497: AddedToken(\"[control_495]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t498: AddedToken(\"[control_496]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t499: AddedToken(\"[control_497]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t500: AddedToken(\"[control_498]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t501: AddedToken(\"[control_499]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t502: AddedToken(\"[control_500]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t503: AddedToken(\"[control_501]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t504: AddedToken(\"[control_502]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t505: AddedToken(\"[control_503]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t506: AddedToken(\"[control_504]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t507: AddedToken(\"[control_505]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t508: AddedToken(\"[control_506]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t509: AddedToken(\"[control_507]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t510: AddedToken(\"[control_508]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t511: AddedToken(\"[control_509]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t512: AddedToken(\"[control_510]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t513: AddedToken(\"[control_511]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t514: AddedToken(\"[control_512]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t515: AddedToken(\"[control_513]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t516: AddedToken(\"[control_514]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t517: AddedToken(\"[control_515]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t518: AddedToken(\"[control_516]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t519: AddedToken(\"[control_517]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t520: AddedToken(\"[control_518]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t521: AddedToken(\"[control_519]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t522: AddedToken(\"[control_520]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t523: AddedToken(\"[control_521]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t524: AddedToken(\"[control_522]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t525: AddedToken(\"[control_523]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t526: AddedToken(\"[control_524]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t527: AddedToken(\"[control_525]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t528: AddedToken(\"[control_526]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t529: AddedToken(\"[control_527]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t530: AddedToken(\"[control_528]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t531: AddedToken(\"[control_529]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t532: AddedToken(\"[control_530]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t533: AddedToken(\"[control_531]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t534: AddedToken(\"[control_532]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t535: AddedToken(\"[control_533]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t536: AddedToken(\"[control_534]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t537: AddedToken(\"[control_535]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t538: AddedToken(\"[control_536]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t539: AddedToken(\"[control_537]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t540: AddedToken(\"[control_538]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t541: AddedToken(\"[control_539]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t542: AddedToken(\"[control_540]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t543: AddedToken(\"[control_541]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t544: AddedToken(\"[control_542]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t545: AddedToken(\"[control_543]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t546: AddedToken(\"[control_544]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t547: AddedToken(\"[control_545]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t548: AddedToken(\"[control_546]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t549: AddedToken(\"[control_547]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t550: AddedToken(\"[control_548]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t551: AddedToken(\"[control_549]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t552: AddedToken(\"[control_550]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t553: AddedToken(\"[control_551]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t554: AddedToken(\"[control_552]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t555: AddedToken(\"[control_553]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t556: AddedToken(\"[control_554]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t557: AddedToken(\"[control_555]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t558: AddedToken(\"[control_556]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t559: AddedToken(\"[control_557]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t560: AddedToken(\"[control_558]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t561: AddedToken(\"[control_559]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t562: AddedToken(\"[control_560]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t563: AddedToken(\"[control_561]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t564: AddedToken(\"[control_562]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t565: AddedToken(\"[control_563]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t566: AddedToken(\"[control_564]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t567: AddedToken(\"[control_565]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t568: AddedToken(\"[control_566]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t569: AddedToken(\"[control_567]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t570: AddedToken(\"[control_568]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t571: AddedToken(\"[control_569]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t572: AddedToken(\"[control_570]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t573: AddedToken(\"[control_571]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t574: AddedToken(\"[control_572]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t575: AddedToken(\"[control_573]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t576: AddedToken(\"[control_574]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t577: AddedToken(\"[control_575]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t578: AddedToken(\"[control_576]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t579: AddedToken(\"[control_577]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t580: AddedToken(\"[control_578]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t581: AddedToken(\"[control_579]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t582: AddedToken(\"[control_580]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t583: AddedToken(\"[control_581]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t584: AddedToken(\"[control_582]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t585: AddedToken(\"[control_583]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t586: AddedToken(\"[control_584]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t587: AddedToken(\"[control_585]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t588: AddedToken(\"[control_586]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t589: AddedToken(\"[control_587]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t590: AddedToken(\"[control_588]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t591: AddedToken(\"[control_589]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t592: AddedToken(\"[control_590]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t593: AddedToken(\"[control_591]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t594: AddedToken(\"[control_592]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t595: AddedToken(\"[control_593]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t596: AddedToken(\"[control_594]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t597: AddedToken(\"[control_595]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t598: AddedToken(\"[control_596]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t599: AddedToken(\"[control_597]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t600: AddedToken(\"[control_598]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t601: AddedToken(\"[control_599]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t602: AddedToken(\"[control_600]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t603: AddedToken(\"[control_601]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t604: AddedToken(\"[control_602]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t605: AddedToken(\"[control_603]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t606: AddedToken(\"[control_604]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t607: AddedToken(\"[control_605]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t608: AddedToken(\"[control_606]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t609: AddedToken(\"[control_607]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t610: AddedToken(\"[control_608]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t611: AddedToken(\"[control_609]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t612: AddedToken(\"[control_610]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t613: AddedToken(\"[control_611]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t614: AddedToken(\"[control_612]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t615: AddedToken(\"[control_613]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t616: AddedToken(\"[control_614]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t617: AddedToken(\"[control_615]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t618: AddedToken(\"[control_616]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t619: AddedToken(\"[control_617]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t620: AddedToken(\"[control_618]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t621: AddedToken(\"[control_619]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t622: AddedToken(\"[control_620]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t623: AddedToken(\"[control_621]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t624: AddedToken(\"[control_622]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t625: AddedToken(\"[control_623]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t626: AddedToken(\"[control_624]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t627: AddedToken(\"[control_625]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t628: AddedToken(\"[control_626]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t629: AddedToken(\"[control_627]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t630: AddedToken(\"[control_628]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t631: AddedToken(\"[control_629]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t632: AddedToken(\"[control_630]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t633: AddedToken(\"[control_631]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t634: AddedToken(\"[control_632]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t635: AddedToken(\"[control_633]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t636: AddedToken(\"[control_634]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t637: AddedToken(\"[control_635]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t638: AddedToken(\"[control_636]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t639: AddedToken(\"[control_637]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t640: AddedToken(\"[control_638]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t641: AddedToken(\"[control_639]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t642: AddedToken(\"[control_640]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t643: AddedToken(\"[control_641]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t644: AddedToken(\"[control_642]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t645: AddedToken(\"[control_643]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t646: AddedToken(\"[control_644]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t647: AddedToken(\"[control_645]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t648: AddedToken(\"[control_646]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t649: AddedToken(\"[control_647]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t650: AddedToken(\"[control_648]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t651: AddedToken(\"[control_649]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t652: AddedToken(\"[control_650]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t653: AddedToken(\"[control_651]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t654: AddedToken(\"[control_652]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t655: AddedToken(\"[control_653]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t656: AddedToken(\"[control_654]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t657: AddedToken(\"[control_655]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t658: AddedToken(\"[control_656]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t659: AddedToken(\"[control_657]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t660: AddedToken(\"[control_658]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t661: AddedToken(\"[control_659]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t662: AddedToken(\"[control_660]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t663: AddedToken(\"[control_661]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t664: AddedToken(\"[control_662]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t665: AddedToken(\"[control_663]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t666: AddedToken(\"[control_664]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t667: AddedToken(\"[control_665]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t668: AddedToken(\"[control_666]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t669: AddedToken(\"[control_667]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t670: AddedToken(\"[control_668]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t671: AddedToken(\"[control_669]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t672: AddedToken(\"[control_670]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t673: AddedToken(\"[control_671]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t674: AddedToken(\"[control_672]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t675: AddedToken(\"[control_673]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t676: AddedToken(\"[control_674]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t677: AddedToken(\"[control_675]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t678: AddedToken(\"[control_676]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t679: AddedToken(\"[control_677]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t680: AddedToken(\"[control_678]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t681: AddedToken(\"[control_679]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t682: AddedToken(\"[control_680]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t683: AddedToken(\"[control_681]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t684: AddedToken(\"[control_682]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t685: AddedToken(\"[control_683]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t686: AddedToken(\"[control_684]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t687: AddedToken(\"[control_685]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t688: AddedToken(\"[control_686]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t689: AddedToken(\"[control_687]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t690: AddedToken(\"[control_688]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t691: AddedToken(\"[control_689]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t692: AddedToken(\"[control_690]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t693: AddedToken(\"[control_691]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t694: AddedToken(\"[control_692]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t695: AddedToken(\"[control_693]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t696: AddedToken(\"[control_694]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t697: AddedToken(\"[control_695]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t698: AddedToken(\"[control_696]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t699: AddedToken(\"[control_697]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t700: AddedToken(\"[control_698]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t701: AddedToken(\"[control_699]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t702: AddedToken(\"[control_700]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t703: AddedToken(\"[control_701]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t704: AddedToken(\"[control_702]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t705: AddedToken(\"[control_703]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t706: AddedToken(\"[control_704]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t707: AddedToken(\"[control_705]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t708: AddedToken(\"[control_706]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t709: AddedToken(\"[control_707]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t710: AddedToken(\"[control_708]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t711: AddedToken(\"[control_709]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t712: AddedToken(\"[control_710]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t713: AddedToken(\"[control_711]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t714: AddedToken(\"[control_712]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t715: AddedToken(\"[control_713]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t716: AddedToken(\"[control_714]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t717: AddedToken(\"[control_715]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t718: AddedToken(\"[control_716]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t719: AddedToken(\"[control_717]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t720: AddedToken(\"[control_718]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t721: AddedToken(\"[control_719]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t722: AddedToken(\"[control_720]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t723: AddedToken(\"[control_721]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t724: AddedToken(\"[control_722]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t725: AddedToken(\"[control_723]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t726: AddedToken(\"[control_724]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t727: AddedToken(\"[control_725]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t728: AddedToken(\"[control_726]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t729: AddedToken(\"[control_727]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t730: AddedToken(\"[control_728]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t731: AddedToken(\"[control_729]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t732: AddedToken(\"[control_730]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t733: AddedToken(\"[control_731]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t734: AddedToken(\"[control_732]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t735: AddedToken(\"[control_733]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t736: AddedToken(\"[control_734]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t737: AddedToken(\"[control_735]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t738: AddedToken(\"[control_736]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t739: AddedToken(\"[control_737]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t740: AddedToken(\"[control_738]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t741: AddedToken(\"[control_739]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t742: AddedToken(\"[control_740]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t743: AddedToken(\"[control_741]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t744: AddedToken(\"[control_742]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t745: AddedToken(\"[control_743]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t746: AddedToken(\"[control_744]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t747: AddedToken(\"[control_745]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t748: AddedToken(\"[control_746]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t749: AddedToken(\"[control_747]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t750: AddedToken(\"[control_748]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t751: AddedToken(\"[control_749]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t752: AddedToken(\"[control_750]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t753: AddedToken(\"[control_751]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t754: AddedToken(\"[control_752]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t755: AddedToken(\"[control_753]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t756: AddedToken(\"[control_754]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t757: AddedToken(\"[control_755]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t758: AddedToken(\"[control_756]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t759: AddedToken(\"[control_757]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t760: AddedToken(\"[control_758]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t761: AddedToken(\"[control_759]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t762: AddedToken(\"[control_760]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t763: AddedToken(\"[control_761]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t764: AddedToken(\"[control_762]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t765: AddedToken(\"[control_763]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t766: AddedToken(\"[control_764]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t767: AddedToken(\"[control_765]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t768: AddedToken(\"[control_766]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t769: AddedToken(\"[control_767]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t770: AddedToken(\"[control_768]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32768: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "Type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "All tokens count: 32769\n",
      "Padding side: left\n",
      "--- Trainer dataset ---\n",
      "Dataset({\n",
      "    features: ['messages', 'input_ids'],\n",
      "    num_rows: 177\n",
      "})\n",
      "[{'content': 'What are some innovative applications enabled by AutoGen?', 'role': 'user'}, {'content': 'Innovative applications enabled by AutoGen include personalized AI tutoring, interactive decision-making in text-based environments, dynamic group chat for collaborative problem-solving, and conversational interfaces for games like chess.', 'role': 'assistant'}]\n",
      "[{'content': 'How does AutoGen enhance agent autonomy in conversations?', 'role': 'user'}, {'content': 'AutoGen enhances agent autonomy in conversations through an auto-reply mechanism, where agents automatically generate and send replies based on received messages unless a termination condition is met.', 'role': 'assistant'}]\n",
      "[{'content': 'How does AutoGen enhance LLM inference features?', 'role': 'user'}, {'content': 'AutoGen enhances LLM inference features with result caching, error handling, message templating, and more via an enhanced LLM inference layer.', 'role': 'assistant'}]\n",
      "[{'content': 'What roles do the AssistantAgent and UserProxyAgent represent in AutoGen?', 'role': 'user'}, {'content': 'In AutoGen, the AssistantAgent acts as an AI assistant backed by LLMs, while the UserProxyAgent acts as a human proxy to solicit human input or execute code/function calls.', 'role': 'assistant'}]\n",
      "[{'content': 'What is the significance of modular workflows in AutoGen?', 'role': 'user'}, {'content': 'The significance of modular workflows in AutoGen lies in the ability to develop, test, and maintain agents independently, simplify the overall development process, and enable flexible and scalable multi-agent interactions.', 'role': 'assistant'}]\n",
      "[{'content': 'What is the role of the AssistantAgent in AutoGen?', 'role': 'user'}, {'content': 'The AssistantAgent in AutoGen acts as an AI assistant backed by LLMs, capable of performing tasks such as generating solutions, providing feedback, and executing code.', 'role': 'assistant'}]\n",
      "[{'content': 'What is AutoGen and what does it aim to achieve?', 'role': 'user'}, {'content': \"AutoGen is an open-source framework designed to enable the development of LLM (Large Language Model) applications through multi-agent conversations. It allows developers to create customizable and conversable agents that can cooperate to accomplish tasks. These agents can be programmed through both natural language and computer code to execute flexible conversation patterns. AutoGen's goal is to serve as a versatile framework for building diverse LLM applications across various complexities and capacities.\", 'role': 'assistant'}]\n",
      "[{'content': 'How does AutoGen enhance the modularity of LLM applications?', 'role': 'user'}, {'content': 'AutoGen enhances the modularity of LLM applications by allowing tasks to be divided into separate agents with specific capabilities, simplifying development and maintenance, and enabling flexible and scalable multi-agent workflows.', 'role': 'assistant'}]\n",
      "[{'content': 'How does AutoGen support dynamic multi-agent conversations?', 'role': 'user'}, {'content': 'AutoGen supports dynamic multi-agent conversations through customized generate_reply functions and function calls, allowing agents to hold conversations while invoking other agents depending on the content and context.', 'role': 'assistant'}]\n",
      "[{'content': 'How can developers customize AutoGen agents?', 'role': 'user'}, {'content': 'Developers can customize AutoGen agents by configuring their backend to support various capabilities, including LLMs, human inputs, and tools. They can also program agents to follow specific conversation patterns and interaction behaviors.', 'role': 'assistant'}]\n",
      "[1, 3, 2592, 1228, 1509, 17595, 9197, 10419, 1254, 12959, 10640, 29572, 4, 24781, 1965, 9197, 10419, 1254, 12959, 10640, 3792, 4095, 2100, 16875, 9079, 5358, 29493, 23737, 5929, 29501, 15495, 1065, 3013, 29501, 6295, 20799, 29493, 11384, 2839, 11474, 1122, 9016, 1965, 3468, 29501, 29481, 19158, 29493, 1072, 6083, 2017, 1559, 10056, 1122, 4665, 1505, 1765, 1587, 29491, 2]\n",
      "[1, 3, 2370, 2003, 12959, 10640, 12744, 8841, 1974, 16399, 1065, 18024, 29572, 4, 12959, 10640, 8818, 3001, 8841, 1974, 16399, 1065, 18024, 1827, 1164, 4795, 29501, 16822, 14943, 29493, 1738, 12535, 10994, 9038, 1072, 4848, 5570, 1265, 3586, 1124, 4642, 9338, 7185, 1032, 2618, 3003, 5412, 1117, 2192, 29491, 2]\n",
      "[1, 3, 2370, 2003, 12959, 10640, 12744, 17472, 29523, 1065, 2871, 4958, 29572, 4, 12959, 10640, 8818, 3001, 17472, 29523, 1065, 2871, 4958, 1163, 1972, 1045, 10796, 29493, 2886, 13620, 29493, 3696, 6520, 1845, 29493, 1072, 1448, 4981, 1164, 22501, 17472, 29523, 1065, 2871, 8255, 29491, 2]\n",
      "[1, 3, 2592, 13194, 1279, 1040, 22399, 15597, 1072, 2015, 12718, 15597, 3672, 1065, 12959, 10640, 29572, 4, 1328, 12959, 10640, 29493, 1040, 22399, 15597, 12668, 1158, 1164, 16875, 14660, 18521, 1254, 17472, 16791, 29493, 2080, 1040, 2015, 12718, 15597, 12668, 1158, 1032, 3698, 16491, 1066, 2896, 19142, 3698, 3555, 1210, 12851, 3464, 29516, 3396, 7238, 29491, 2]\n",
      "[1, 3, 2592, 1117, 1040, 19077, 1070, 1736, 1866, 1539, 1902, 4379, 1065, 12959, 10640, 29572, 4, 1183, 19077, 1070, 1736, 1866, 1539, 1902, 4379, 1065, 12959, 10640, 11195, 1065, 1040, 6305, 1066, 2718, 29493, 2137, 29493, 1072, 9855, 12535, 23741, 29493, 10646, 2343, 1040, 8312, 4867, 2527, 29493, 1072, 9002, 18342, 1072, 11199, 1290, 6847, 29501, 15470, 15751, 29491, 2]\n",
      "[1, 3, 2592, 1117, 1040, 4673, 1070, 1040, 22399, 15597, 1065, 12959, 10640, 29572, 4, 1183, 22399, 15597, 1065, 12959, 10640, 12668, 1158, 1164, 16875, 14660, 18521, 1254, 17472, 16791, 29493, 11405, 1070, 14569, 10564, 2027, 1158, 21133, 7919, 29493, 8269, 12907, 29493, 1072, 5314, 1056, 3464, 29491, 2]\n",
      "[1, 3, 2592, 1117, 12959, 10640, 1072, 1535, 2003, 1146, 7271, 1066, 7387, 29572, 4, 12959, 10640, 1117, 1164, 2333, 29501, 2162, 11550, 6450, 1066, 9002, 1040, 4867, 1070, 17472, 29523, 1093, 28176, 16357, 9639, 29499, 9197, 1827, 6847, 29501, 15470, 18024, 29491, 1429, 6744, 18660, 1066, 2999, 3228, 12320, 1072, 6083, 1290, 12535, 1137, 1309, 19236, 1148, 1066, 18468, 10564, 29491, 3725, 12535, 1309, 1115, 2775, 2359, 1827, 2328, 4997, 4610, 1072, 6842, 3464, 1066, 12851, 18342, 7882, 12301, 29491, 12959, 10640, 29510, 29481, 6309, 1117, 1066, 7799, 1158, 1032, 4270, 14259, 11550, 1122, 4435, 13604, 17472, 29523, 9197, 3441, 4886, 5398, 1986, 1072, 17793, 1986, 29491, 2]\n",
      "[1, 3, 2370, 2003, 12959, 10640, 12744, 1040, 1736, 1866, 1240, 1070, 17472, 29523, 9197, 29572, 4, 12959, 10640, 8818, 3001, 1040, 1736, 1866, 1240, 1070, 17472, 29523, 9197, 1254, 10604, 10564, 1066, 1115, 14338, 1546, 8449, 12535, 1163, 3716, 17353, 29493, 10646, 6987, 4867, 1072, 13132, 29493, 1072, 26516, 18342, 1072, 11199, 1290, 6847, 29501, 15470, 1539, 1902, 4379, 29491, 2]\n",
      "[1, 3, 2370, 2003, 12959, 10640, 2528, 11384, 6847, 29501, 15470, 18024, 29572, 4, 12959, 10640, 12330, 11384, 6847, 29501, 15470, 18024, 1827, 3228, 2100, 9038, 29498, 16822, 6340, 1072, 1676, 7238, 29493, 10604, 12535, 1066, 3168, 18024, 2080, 2072, 9760, 1567, 12535, 10853, 1124, 1040, 3804, 1072, 3526, 29491, 2]\n",
      "[1, 3, 2370, 1309, 18660, 3228, 1421, 12959, 10640, 12535, 29572, 4, 9355, 1172, 1309, 3228, 1421, 12959, 10640, 12535, 1254, 3471, 2146, 1420, 25095, 1066, 2528, 4886, 17353, 29493, 3258, 17472, 16791, 29493, 3698, 15159, 29493, 1072, 7808, 29491, 2074, 1309, 1603, 2775, 12535, 1066, 2140, 3716, 7882, 12301, 1072, 11954, 24729, 29491, 2]\n",
      "--- Trainer data collation ---\n",
      "DataCollatorForLanguageModeling(pad_token_id=32768, completion_only_loss=False, padding_free=False, pad_to_multiple_of=None, return_tensors='pt')\n",
      "{'input_ids': tensor([[    1,     3,  2592,  ..., 32768, 32768, 32768],\n",
      "        [    1,     3,  2370,  ..., 32768, 32768, 32768],\n",
      "        [    1,     3,  2370,  ..., 32768, 32768, 32768],\n",
      "        ...,\n",
      "        [    1,     3,  2592,  ..., 32768, 32768, 32768],\n",
      "        [    1,     3,  2592,  ..., 32768, 32768, 32768],\n",
      "        [    1,     3,  2370,  ..., 32768, 32768, 32768]]), 'labels': tensor([[   1,    3, 2592,  ..., -100, -100, -100],\n",
      "        [   1,    3, 2370,  ..., -100, -100, -100],\n",
      "        [   1,    3, 2370,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [   1,    3, 2592,  ..., -100, -100, -100],\n",
      "        [   1,    3, 2592,  ..., -100, -100, -100],\n",
      "        [   1,    3, 2370,  ..., -100, -100, -100]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([    1,     3,  2592,  1228,  1509, 17595,  9197, 10419,  1254, 12959,\n",
      "        10640, 29572,     4, 24781,  1965,  9197, 10419,  1254, 12959, 10640,\n",
      "         3792,  4095,  2100, 16875,  9079,  5358, 29493, 23737,  5929, 29501,\n",
      "        15495,  1065,  3013, 29501,  6295, 20799, 29493, 11384,  2839, 11474,\n",
      "         1122,  9016,  1965,  3468, 29501, 29481, 19158, 29493,  1072,  6083,\n",
      "         2017,  1559, 10056,  1122,  4665,  1505,  1765,  1587, 29491,     2,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2370,  2003, 12959, 10640, 12744,  8841,  1974, 16399,\n",
      "         1065, 18024, 29572,     4, 12959, 10640,  8818,  3001,  8841,  1974,\n",
      "        16399,  1065, 18024,  1827,  1164,  4795, 29501, 16822, 14943, 29493,\n",
      "         1738, 12535, 10994,  9038,  1072,  4848,  5570,  1265,  3586,  1124,\n",
      "         4642,  9338,  7185,  1032,  2618,  3003,  5412,  1117,  2192, 29491,\n",
      "            2, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2370,  2003, 12959, 10640, 12744, 17472, 29523,  1065,\n",
      "         2871,  4958, 29572,     4, 12959, 10640,  8818,  3001, 17472, 29523,\n",
      "         1065,  2871,  4958,  1163,  1972,  1045, 10796, 29493,  2886, 13620,\n",
      "        29493,  3696,  6520,  1845, 29493,  1072,  1448,  4981,  1164, 22501,\n",
      "        17472, 29523,  1065,  2871,  8255, 29491,     2, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2592, 13194,  1279,  1040, 22399, 15597,  1072,  2015,\n",
      "        12718, 15597,  3672,  1065, 12959, 10640, 29572,     4,  1328, 12959,\n",
      "        10640, 29493,  1040, 22399, 15597, 12668,  1158,  1164, 16875, 14660,\n",
      "        18521,  1254, 17472, 16791, 29493,  2080,  1040,  2015, 12718, 15597,\n",
      "        12668,  1158,  1032,  3698, 16491,  1066,  2896, 19142,  3698,  3555,\n",
      "         1210, 12851,  3464, 29516,  3396,  7238, 29491,     2, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2592,  1117,  1040, 19077,  1070,  1736,  1866,  1539,\n",
      "         1902,  4379,  1065, 12959, 10640, 29572,     4,  1183, 19077,  1070,\n",
      "         1736,  1866,  1539,  1902,  4379,  1065, 12959, 10640, 11195,  1065,\n",
      "         1040,  6305,  1066,  2718, 29493,  2137, 29493,  1072,  9855, 12535,\n",
      "        23741, 29493, 10646,  2343,  1040,  8312,  4867,  2527, 29493,  1072,\n",
      "         9002, 18342,  1072, 11199,  1290,  6847, 29501, 15470, 15751, 29491,\n",
      "            2, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2592,  1117,  1040,  4673,  1070,  1040, 22399, 15597,\n",
      "         1065, 12959, 10640, 29572,     4,  1183, 22399, 15597,  1065, 12959,\n",
      "        10640, 12668,  1158,  1164, 16875, 14660, 18521,  1254, 17472, 16791,\n",
      "        29493, 11405,  1070, 14569, 10564,  2027,  1158, 21133,  7919, 29493,\n",
      "         8269, 12907, 29493,  1072,  5314,  1056,  3464, 29491,     2, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2592,  1117, 12959, 10640,  1072,  1535,  2003,  1146,\n",
      "         7271,  1066,  7387, 29572,     4, 12959, 10640,  1117,  1164,  2333,\n",
      "        29501,  2162, 11550,  6450,  1066,  9002,  1040,  4867,  1070, 17472,\n",
      "        29523,  1093, 28176, 16357,  9639, 29499,  9197,  1827,  6847, 29501,\n",
      "        15470, 18024, 29491,  1429,  6744, 18660,  1066,  2999,  3228, 12320,\n",
      "         1072,  6083,  1290, 12535,  1137,  1309, 19236,  1148,  1066, 18468,\n",
      "        10564, 29491,  3725, 12535,  1309,  1115,  2775,  2359,  1827,  2328,\n",
      "         4997,  4610,  1072,  6842,  3464,  1066, 12851, 18342,  7882, 12301,\n",
      "        29491, 12959, 10640, 29510, 29481,  6309,  1117,  1066,  7799,  1158,\n",
      "         1032,  4270, 14259, 11550,  1122,  4435, 13604, 17472, 29523,  9197,\n",
      "         3441,  4886,  5398,  1986,  1072, 17793,  1986, 29491,     2, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2370,  2003, 12959, 10640, 12744,  1040,  1736,  1866,\n",
      "         1240,  1070, 17472, 29523,  9197, 29572,     4, 12959, 10640,  8818,\n",
      "         3001,  1040,  1736,  1866,  1240,  1070, 17472, 29523,  9197,  1254,\n",
      "        10604, 10564,  1066,  1115, 14338,  1546,  8449, 12535,  1163,  3716,\n",
      "        17353, 29493, 10646,  6987,  4867,  1072, 13132, 29493,  1072, 26516,\n",
      "        18342,  1072, 11199,  1290,  6847, 29501, 15470,  1539,  1902,  4379,\n",
      "        29491,     2, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2370,  2003, 12959, 10640,  2528, 11384,  6847, 29501,\n",
      "        15470, 18024, 29572,     4, 12959, 10640, 12330, 11384,  6847, 29501,\n",
      "        15470, 18024,  1827,  3228,  2100,  9038, 29498, 16822,  6340,  1072,\n",
      "         1676,  7238, 29493, 10604, 12535,  1066,  3168, 18024,  2080,  2072,\n",
      "         9760,  1567, 12535, 10853,  1124,  1040,  3804,  1072,  3526, 29491,\n",
      "            2, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2370,  1309, 18660,  3228,  1421, 12959, 10640, 12535,\n",
      "        29572,     4,  9355,  1172,  1309,  3228,  1421, 12959, 10640, 12535,\n",
      "         1254,  3471,  2146,  1420, 25095,  1066,  2528,  4886, 17353, 29493,\n",
      "         3258, 17472, 16791, 29493,  3698, 15159, 29493,  1072,  7808, 29491,\n",
      "         2074,  1309,  1603,  2775, 12535,  1066,  2140,  3716,  7882, 12301,\n",
      "         1072, 11954, 24729, 29491,     2, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768,\n",
      "        32768, 32768, 32768, 32768, 32768, 32768, 32768])\n",
      "tensor([    1,     3,  2592,  1228,  1509, 17595,  9197, 10419,  1254, 12959,\n",
      "        10640, 29572,     4, 24781,  1965,  9197, 10419,  1254, 12959, 10640,\n",
      "         3792,  4095,  2100, 16875,  9079,  5358, 29493, 23737,  5929, 29501,\n",
      "        15495,  1065,  3013, 29501,  6295, 20799, 29493, 11384,  2839, 11474,\n",
      "         1122,  9016,  1965,  3468, 29501, 29481, 19158, 29493,  1072,  6083,\n",
      "         2017,  1559, 10056,  1122,  4665,  1505,  1765,  1587, 29491,     2,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2370,  2003, 12959, 10640, 12744,  8841,  1974, 16399,\n",
      "         1065, 18024, 29572,     4, 12959, 10640,  8818,  3001,  8841,  1974,\n",
      "        16399,  1065, 18024,  1827,  1164,  4795, 29501, 16822, 14943, 29493,\n",
      "         1738, 12535, 10994,  9038,  1072,  4848,  5570,  1265,  3586,  1124,\n",
      "         4642,  9338,  7185,  1032,  2618,  3003,  5412,  1117,  2192, 29491,\n",
      "            2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2370,  2003, 12959, 10640, 12744, 17472, 29523,  1065,\n",
      "         2871,  4958, 29572,     4, 12959, 10640,  8818,  3001, 17472, 29523,\n",
      "         1065,  2871,  4958,  1163,  1972,  1045, 10796, 29493,  2886, 13620,\n",
      "        29493,  3696,  6520,  1845, 29493,  1072,  1448,  4981,  1164, 22501,\n",
      "        17472, 29523,  1065,  2871,  8255, 29491,     2,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2592, 13194,  1279,  1040, 22399, 15597,  1072,  2015,\n",
      "        12718, 15597,  3672,  1065, 12959, 10640, 29572,     4,  1328, 12959,\n",
      "        10640, 29493,  1040, 22399, 15597, 12668,  1158,  1164, 16875, 14660,\n",
      "        18521,  1254, 17472, 16791, 29493,  2080,  1040,  2015, 12718, 15597,\n",
      "        12668,  1158,  1032,  3698, 16491,  1066,  2896, 19142,  3698,  3555,\n",
      "         1210, 12851,  3464, 29516,  3396,  7238, 29491,     2,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2592,  1117,  1040, 19077,  1070,  1736,  1866,  1539,\n",
      "         1902,  4379,  1065, 12959, 10640, 29572,     4,  1183, 19077,  1070,\n",
      "         1736,  1866,  1539,  1902,  4379,  1065, 12959, 10640, 11195,  1065,\n",
      "         1040,  6305,  1066,  2718, 29493,  2137, 29493,  1072,  9855, 12535,\n",
      "        23741, 29493, 10646,  2343,  1040,  8312,  4867,  2527, 29493,  1072,\n",
      "         9002, 18342,  1072, 11199,  1290,  6847, 29501, 15470, 15751, 29491,\n",
      "            2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2592,  1117,  1040,  4673,  1070,  1040, 22399, 15597,\n",
      "         1065, 12959, 10640, 29572,     4,  1183, 22399, 15597,  1065, 12959,\n",
      "        10640, 12668,  1158,  1164, 16875, 14660, 18521,  1254, 17472, 16791,\n",
      "        29493, 11405,  1070, 14569, 10564,  2027,  1158, 21133,  7919, 29493,\n",
      "         8269, 12907, 29493,  1072,  5314,  1056,  3464, 29491,     2,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2592,  1117, 12959, 10640,  1072,  1535,  2003,  1146,\n",
      "         7271,  1066,  7387, 29572,     4, 12959, 10640,  1117,  1164,  2333,\n",
      "        29501,  2162, 11550,  6450,  1066,  9002,  1040,  4867,  1070, 17472,\n",
      "        29523,  1093, 28176, 16357,  9639, 29499,  9197,  1827,  6847, 29501,\n",
      "        15470, 18024, 29491,  1429,  6744, 18660,  1066,  2999,  3228, 12320,\n",
      "         1072,  6083,  1290, 12535,  1137,  1309, 19236,  1148,  1066, 18468,\n",
      "        10564, 29491,  3725, 12535,  1309,  1115,  2775,  2359,  1827,  2328,\n",
      "         4997,  4610,  1072,  6842,  3464,  1066, 12851, 18342,  7882, 12301,\n",
      "        29491, 12959, 10640, 29510, 29481,  6309,  1117,  1066,  7799,  1158,\n",
      "         1032,  4270, 14259, 11550,  1122,  4435, 13604, 17472, 29523,  9197,\n",
      "         3441,  4886,  5398,  1986,  1072, 17793,  1986, 29491,     2,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2370,  2003, 12959, 10640, 12744,  1040,  1736,  1866,\n",
      "         1240,  1070, 17472, 29523,  9197, 29572,     4, 12959, 10640,  8818,\n",
      "         3001,  1040,  1736,  1866,  1240,  1070, 17472, 29523,  9197,  1254,\n",
      "        10604, 10564,  1066,  1115, 14338,  1546,  8449, 12535,  1163,  3716,\n",
      "        17353, 29493, 10646,  6987,  4867,  1072, 13132, 29493,  1072, 26516,\n",
      "        18342,  1072, 11199,  1290,  6847, 29501, 15470,  1539,  1902,  4379,\n",
      "        29491,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2370,  2003, 12959, 10640,  2528, 11384,  6847, 29501,\n",
      "        15470, 18024, 29572,     4, 12959, 10640, 12330, 11384,  6847, 29501,\n",
      "        15470, 18024,  1827,  3228,  2100,  9038, 29498, 16822,  6340,  1072,\n",
      "         1676,  7238, 29493, 10604, 12535,  1066,  3168, 18024,  2080,  2072,\n",
      "         9760,  1567, 12535, 10853,  1124,  1040,  3804,  1072,  3526, 29491,\n",
      "            2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([    1,     3,  2370,  1309, 18660,  3228,  1421, 12959, 10640, 12535,\n",
      "        29572,     4,  9355,  1172,  1309,  3228,  1421, 12959, 10640, 12535,\n",
      "         1254,  3471,  2146,  1420, 25095,  1066,  2528,  4886, 17353, 29493,\n",
      "         3258, 17472, 16791, 29493,  3698, 15159, 29493,  1072,  7808, 29491,\n",
      "         2074,  1309,  1603,  2775, 12535,  1066,  2140,  3716,  7882, 12301,\n",
      "         1072, 11954, 24729, 29491,     2,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Trainer model ---\")\n",
    "print(trainer.model)\n",
    "print(\"Config:\", trainer.model.config)\n",
    "print(\"Generation Config:\", trainer.model.generation_config)\n",
    "\n",
    "print(\"Get Trainable Parameters\")\n",
    "print(trainer.model.print_trainable_parameters())\n",
    "# trainable params: 167,772,160 || all params: 7,415,803,904 || trainable%: 2.2624\n",
    "\n",
    "print(\"--- Trainer tokenizer ---\")\n",
    "print(trainer.processing_class)\n",
    "print(\"Type:\", type(trainer.processing_class))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", trainer.processing_class.special_tokens_map)\n",
    "print(\"All tokens count:\", len(trainer.processing_class))\n",
    "print(\"Padding side:\", trainer.processing_class.padding_side)\n",
    "\n",
    "\n",
    "print(\"--- Trainer dataset ---\")\n",
    "print(trainer.train_dataset)\n",
    "\n",
    "for t in trainer.train_dataset[\"messages\"][:10]:\n",
    "    print(t)\n",
    "\n",
    "for t in trainer.train_dataset[\"input_ids\"][:10]:\n",
    "    print(t)\n",
    "\n",
    "\n",
    "print(\"--- Trainer data collation ---\")\n",
    "print(trainer.data_collator)\n",
    "collated_data = trainer.data_collator(trainer.train_dataset)\n",
    "print(collated_data)\n",
    "\n",
    "for t in collated_data[\"input_ids\"][:10]:\n",
    "    print(t)\n",
    "\n",
    "for t in collated_data[\"labels\"][:10]:\n",
    "    print(t)\n",
    "\n",
    "for t in collated_data[\"attention_mask\"][:10]:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters logged to TensorBoard\n"
     ]
    }
   ],
   "source": [
    "# --- LOG HYPERPARAMETERS TO TENSORBOARD ---\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=logging_dir)\n",
    "\n",
    "# Create markdown summary of hyperparameters\n",
    "hyperparams_summary = f\"\"\"\n",
    "# Training Run: {run_name}\n",
    "\n",
    "## Run Information\n",
    "- **Timestamp**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- **Model**: {model_name}\n",
    "- **Dataset**: lukaskellerstein/autogen\n",
    "- **Train samples**: {len(final_datasets[\"train\"])}\n",
    "- **Eval samples**: {len(final_datasets[\"test\"])}\n",
    "\n",
    "## Model Configuration\n",
    "- **Quantization**: 4-bit (NF4)\n",
    "- **Compute dtype**: float16\n",
    "- **Double quantization**: True\n",
    "- **Base vocab size**: 32768\n",
    "- **Extended vocab size**: {base_model.config.vocab_size}\n",
    "- **Pad token ID**: {base_tokenizer.pad_token_id}\n",
    "\n",
    "## LoRA/PEFT Configuration\n",
    "- **LoRA rank (r)**: {peft_config.r}\n",
    "- **LoRA alpha**: {peft_config.lora_alpha}\n",
    "- **LoRA dropout**: {peft_config.lora_dropout}\n",
    "- **Target modules**: {peft_config.target_modules}\n",
    "- **Bias**: {peft_config.bias}\n",
    "- **Task type**: {peft_config.task_type}\n",
    "\n",
    "## Training Hyperparameters\n",
    "- **Learning rate**: {lr}\n",
    "- **Batch size**: {bs}\n",
    "- **Gradient accumulation steps**: {ga_steps}\n",
    "- **Effective batch size**: {bs * ga_steps}\n",
    "- **Epochs**: {epochs}\n",
    "- **Steps per epoch**: {steps_per_epoch}\n",
    "- **Total training steps**: {steps_per_epoch * epochs}\n",
    "- **LR scheduler**: {training_args.lr_scheduler_type}\n",
    "- **FP16**: {training_args.fp16}\n",
    "- **Eval strategy**: {training_args.eval_strategy}\n",
    "- **Eval steps**: {training_args.eval_steps}\n",
    "- **Logging steps**: {training_args.logging_steps}\n",
    "- **Save steps**: {training_args.save_steps}\n",
    "- **Save total limit**: {training_args.save_total_limit}\n",
    "\n",
    "## Directories\n",
    "- **Output dir**: {output_dir}\n",
    "- **Logging dir**: {logging_dir}\n",
    "\"\"\"\n",
    "\n",
    "writer.add_text(\"Hyperparameters\", hyperparams_summary, 0)\n",
    "writer.close()\n",
    "\n",
    "print(\"✓ Hyperparameters logged to TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 09:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.772400</td>\n",
       "      <td>1.537762</td>\n",
       "      <td>1.485070</td>\n",
       "      <td>10437.000000</td>\n",
       "      <td>0.662592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.066600</td>\n",
       "      <td>1.252368</td>\n",
       "      <td>1.209841</td>\n",
       "      <td>20630.000000</td>\n",
       "      <td>0.693283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.696700</td>\n",
       "      <td>1.216455</td>\n",
       "      <td>0.886209</td>\n",
       "      <td>30969.000000</td>\n",
       "      <td>0.721956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.471600</td>\n",
       "      <td>1.326223</td>\n",
       "      <td>0.715053</td>\n",
       "      <td>41183.000000</td>\n",
       "      <td>0.729845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.372300</td>\n",
       "      <td>1.381443</td>\n",
       "      <td>0.674828</td>\n",
       "      <td>51380.000000</td>\n",
       "      <td>0.728997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training takes: 0:09:56.767539\n",
      "Total takes: 0:09:56.768095\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(\"Start training...\")\n",
    "startTrain = time.time()\n",
    "trainer.train()\n",
    "td = timedelta(seconds=(time.time() - startTrain))\n",
    "print(f\"Training takes: {td}\")\n",
    "\n",
    "\n",
    "# Total time for the script\n",
    "td = timedelta(seconds=(time.time() - start))\n",
    "print(f\"Total takes: {td}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the adapter - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Tell me a joke about programmers.\n",
      "Response: Why don't programmers like nature? Because it has too many bugs.\n",
      "\n",
      "Prompt: What is Autogen?\n",
      "Response: Autogen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks.\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, tokenizer, prompt):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt as a conversation\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens (skip the input)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test 1 - simple prompt\n",
    "prompt = \"Tell me a joke about programmers.\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(base_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Test 2 - Autogen related prompt\n",
    "prompt = \"What is Autogen?\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(base_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the adapter (to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SAVED_ADAPTER/tokenizer_config.json',\n",
       " 'SAVED_ADAPTER/special_tokens_map.json',\n",
       " 'SAVED_ADAPTER/tokenizer.model',\n",
       " 'SAVED_ADAPTER/added_tokens.json',\n",
       " 'SAVED_ADAPTER/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"SAVED_ADAPTER\")\n",
    "trainer.processing_class.save_pretrained(\"SAVED_ADAPTER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push adapter (to hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7c251150b64aa8b132819d7e9f2026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737dbdf2ce0746378ddffd3b9f9b6777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lukaskellerstein/autogen-mistral-4bit-lora-adapter/commit/391d0b23c746eca2094f461985a9ac07c0b96ce9', commit_message='Upload tokenizer', commit_description='', oid='391d0b23c746eca2094f461985a9ac07c0b96ce9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/autogen-mistral-4bit-lora-adapter\",\n",
    "    token=API_TOKEN,\n",
    ")\n",
    "trainer.processing_class.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/autogen-mistral-4bit-lora-adapter\",\n",
    "    token=API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGED model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge LoRA adapter and base model => merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aa8be599304b92ab156fd0a031be87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(32768, 4096)\n",
      "Linear(in_features=4096, out_features=32768, bias=False)\n",
      "Model Vocabulary Size: 32768\n",
      "before 32768\n",
      "after 32769\n",
      "before 2 None\n",
      "after 2 32768\n",
      "Model Vocabulary Size: 32768\n",
      "Model Vocabulary Size: 32769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aaf9fb88e79480794517a30934b51a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a034080d358d4281937d83603984238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32769, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[    1,     3,  2592,  1117, 12959, 10640,  1065, 12344, 29572, 29473,\n",
      "             4]], device='cuda:0')\n",
      "<s>[INST] What is AutoGen in abstract? [/INST]AutoGen in abstract is a framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloading and merging model: 100%|██████████| 678/678 [00:00<00:00, 5534.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Merge LoRA adapters with base model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "adapter_path = \"lukaskellerstein/autogen-mistral-4bit-lora-adapter\"  # input: adapters\n",
    "\n",
    "# ------------------------------------------------\n",
    "# WE CANNOT MERGE Quantized model with LoRA !!!!!!!!!!!!!\n",
    "# ------------------------------------------------\n",
    "# Model\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "print(base_model.get_input_embeddings())\n",
    "print(base_model.get_output_embeddings())\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"before\", len(base_tokenizer))\n",
    "base_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "print(\"after\", len(base_tokenizer))\n",
    "\n",
    "print(\"before 2\", base_model.config.pad_token_id)\n",
    "base_model.config.pad_token_id = base_tokenizer.pad_token_id\n",
    "base_model.generation_config.pad_token_id = base_tokenizer.pad_token_id\n",
    "print(\"after 2\", base_model.config.pad_token_id)\n",
    "\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "base_model.resize_token_embeddings(len(base_tokenizer))\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "\n",
    "\n",
    "# Load PEFT model\n",
    "peft_model_loaded = PeftModel.from_pretrained(\n",
    "    model=base_model,\n",
    "    model_id=adapter_path,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "print(type(peft_model_loaded))\n",
    "print(peft_model_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the base model + adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, tokenizer, prompt):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt as a conversation\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens (skip the input)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "prompt = \"Tell me a joke about programmers.\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(peft_model_loaded, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the model !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Merge base model and LoRA adapter together into one full model\n",
    "# ---------------------------------------------------------------\n",
    "merged_model = peft_model_loaded.merge_and_unload(progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n",
      "Architecture: MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32769, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 32769\n",
      "Input embeddings:\n",
      "Embedding(32769, 4096)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=32769, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "All tokens count: 32769\n",
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(merged_model))\n",
    "print(\"Architecture:\", merged_model)\n",
    "print(\"Config:\", merged_model.config)\n",
    "print(\"Generation Config:\", merged_model.generation_config)\n",
    "print(\"Model Vocabulary Size:\", merged_model.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(merged_model.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(merged_model.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(base_tokenizer))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", base_tokenizer.special_tokens_map)\n",
    "print(\"All tokens count:\", len(base_tokenizer))\n",
    "print(\"Padding side:\", base_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test \"merged\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, tokenizer, prompt):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt as a conversation\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens (skip the input)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Test 1 - simple prompt\n",
    "prompt = \"Tell me a joke about programmers.\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(merged_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Test 2 - tool use prompt\n",
    "prompt = \"The system should select a function from the following list if it can solve the user's question: [{'type': 'function', 'function': {'name': 'get_natural_phenomenon_details', 'description': 'Get details about a specific natural phenomenon', 'parameters': {'type': 'object', 'properties': {'phenomenon_name': {'type': 'string', 'description': 'The name of the phenomenon'}}, 'required': ['phenomenon_name']}}}, {'type': 'function', 'function': {'name': 'get_top_natural_phenomena', 'description': 'Get the names of the top N natural phenomena by scientific interest', 'parameters': {'type': 'object', 'properties': {'number': {'type': 'integer', 'description': 'The number of top natural phenomena to get'}}, 'required': ['number']}}}]. If calling a function can answer the question, return only a JSON with the call of that function. If not, return the answer as usual./nWhat is the Aurora Borealis?\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(merged_model, base_tokenizer, prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save \"merged\" model (to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MERGED/tokenizer_config.json',\n",
       " 'MERGED/special_tokens_map.json',\n",
       " 'MERGED/tokenizer.model',\n",
       " 'MERGED/added_tokens.json',\n",
       " 'MERGED/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.save_pretrained(\"MERGED\")\n",
    "\n",
    "base_tokenizer.save_pretrained(\"MERGED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push \"merged\" model (to hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b40e5946044cc58569163480ac0aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1198e06ee3044e22b7f2e8604977e746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3095d70f289417dae63bbbf23629ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb3c50b17b44ccbb6c807fe45b7ae05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafa37af455842eba1c90193998f4833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd03458c85d4d06b715ba254b178430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lukaskellerstein/autogen-mistral-16bit-merged/commit/193ebadf5fcfeb8df01c915820a1109d20b28cc7', commit_message='Upload tokenizer', commit_description='', oid='193ebadf5fcfeb8df01c915820a1109d20b28cc7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/autogen-mistral-16bit-merged\",\n",
    "    token=API_TOKEN,\n",
    ")\n",
    "base_tokenizer.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/autogen-mistral-16bit-merged\",\n",
    "    token=API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged model from HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_hub_name = \"lukaskellerstein/autogen-mistral-16bit-merged\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fc59c5d35744e1b80f8fcdbbd43133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979e73af1cad44b384e916730f631922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3de2635292a4409a438bee8feea041c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d766f2bb30645f9919f3cfe53f14145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cec285b8b94484a92daa22ee99a9d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f9149c7fca4d0193bbd781cf6d8b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model_loaded_fromHF = AutoModelForCausalLM.from_pretrained(\n",
    "    model_from_hub_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "tokenizer_loaded_fromHF = AutoTokenizer.from_pretrained(model_from_hub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n",
      "Architecture: MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32769, 4096, padding_idx=32768)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"_name_or_path\": \"lukaskellerstein/autogen-mistral-16bit-merged\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 32769\n",
      "Input embeddings:\n",
      "Embedding(32769, 4096, padding_idx=32768)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=32769, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "All tokens count: 32769\n",
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(model_loaded_fromHF))\n",
    "print(\"Architecture:\", model_loaded_fromHF)\n",
    "print(\"Config:\", model_loaded_fromHF.config)\n",
    "print(\"Generation Config:\", model_loaded_fromHF.generation_config)\n",
    "print(\"Model Vocabulary Size:\", model_loaded_fromHF.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(model_loaded_fromHF.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(model_loaded_fromHF.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(tokenizer_loaded_fromHF))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", tokenizer_loaded_fromHF.special_tokens_map)\n",
    "print(\"All tokens count:\", len(tokenizer_loaded_fromHF))\n",
    "print(\"Padding side:\", tokenizer_loaded_fromHF.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, tokenizer, prompt):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt as a conversation\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens (skip the input)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test 1 - simple prompt\n",
    "prompt = \"Tell me a joke about programmers.\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(model_loaded_fromHF, tokenizer_loaded_fromHF, prompt)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Test 2 - tool use prompt\n",
    "prompt = \"The system should select a function from the following list if it can solve the user's question: [{'type': 'function', 'function': {'name': 'get_natural_phenomenon_details', 'description': 'Get details about a specific natural phenomenon', 'parameters': {'type': 'object', 'properties': {'phenomenon_name': {'type': 'string', 'description': 'The name of the phenomenon'}}, 'required': ['phenomenon_name']}}}, {'type': 'function', 'function': {'name': 'get_top_natural_phenomena', 'description': 'Get the names of the top N natural phenomena by scientific interest', 'parameters': {'type': 'object', 'properties': {'number': {'type': 'integer', 'description': 'The number of top natural phenomena to get'}}, 'required': ['number']}}}]. If calling a function can answer the question, return only a JSON with the call of that function. If not, return the answer as usual./nWhat is the Aurora Borealis?\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "response = test_model(model_loaded_fromHF, tokenizer_loaded_fromHF, prompt)\n",
    "print(f\"Response: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
