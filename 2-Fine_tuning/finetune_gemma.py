"""
FINE-TUNING GEMMA 2-2B-IT PRO FLEURDIN
=======================================

Tento skript fine-tunuje Gemma 2-2B-it model na datasetu TomasBo/Fleurdin
s 2,281 Q&A p√°ry o esenci√°ln√≠ch olej√≠ch.

Pou≈æ√≠v√°:
- LoRA (Low-Rank Adaptation) pro efektivn√≠ training
- 4-bit quantization (QLoRA) pro √∫sporu pamƒõti
- HuggingFace Transformers + PEFT

Hardware requirements:
- GPU: 12GB+ VRAM (RTX 3060+, T4, A10G)
- RAM: 16GB+
- Disk: 20GB free

Alternativnƒõ: Google Colab (free tier s T4 GPU)
"""

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datetime import datetime

# ============================================
# KONFIGURACE
# ============================================

# Model
BASE_MODEL = "google/gemma-2-2b-it"
OUTPUT_DIR = "./fleurdin-gemma-2b"  # Lok√°ln√≠ v√Ωstup
HF_REPO_NAME = "TomasBo/fleurdin-gemma-2b"  # HuggingFace repo (pro upload)

# Dataset
DATASET_NAME = "TomasBo/Fleurdin"

# Training hyperparameters
TRAINING_CONFIG = {
    "num_train_epochs": 3,           # Poƒçet epoch (3 je dobr√Ω start)
    "per_device_train_batch_size": 4, # Batch size (4 pro 12GB GPU)
    "gradient_accumulation_steps": 4, # = effective batch 16
    "learning_rate": 2e-4,            # Learning rate pro LoRA
    "warmup_steps": 100,              # Warmup
    "logging_steps": 10,              # Log ka≈æd√Ωch 10 krok≈Ø
    "save_steps": 100,                # Save checkpoint ka≈æd√Ωch 100 krok≈Ø
    "max_steps": -1,                  # -1 = train cel√Ω dataset
}

# LoRA config (pro memory efficient training)
LORA_CONFIG = {
    "r": 16,                    # Rank (16 je standard)
    "lora_alpha": 32,           # Alpha (2x rank)
    "target_modules": [         # Kter√© layers trainovat
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    "lora_dropout": 0.05,       # Dropout
    "bias": "none",
    "task_type": "CAUSAL_LM"
}


# ============================================
# P≈ò√çPRAVA DATASETU
# ============================================

def prepare_dataset(dataset_name):
    """Naƒçti a p≈ôiprav dataset pro training"""

    print(f"\nüì• Naƒç√≠t√°m dataset: {dataset_name}")
    dataset = load_dataset(dataset_name)

    print(f"‚úÖ Dataset naƒçten!")
    print(f"   Poƒçet training p≈ô√≠klad≈Ø: {len(dataset['train'])}")

    return dataset


def format_chat_for_gemma(example, tokenizer):
    """
    Form√°tuj konverzaci pro Gemma 2-2B-it chat format

    Input example:
    {
      "text": [
        {"role": "user", "content": "Jak√© jsou √∫ƒçinky oregana?"},
        {"role": "assistant", "content": "Oregano m√° tyto √∫ƒçinky..."}
      ]
    }

    Output: Tokenizovan√Ω text v Gemma form√°tu
    """

    # Gemma chat template
    messages = example["text"]

    # Apply chat template
    formatted_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False  # U≈æ m√°me assistant odpovƒõƒè
    )

    # Tokenize
    tokenized = tokenizer(
        formatted_text,
        truncation=True,
        max_length=512,  # Max d√©lka (Gemma podporuje a≈æ 8k, ale 512 staƒç√≠)
        padding=False
    )

    # Labels = stejn√© jako input_ids (pro causal LM)
    tokenized["labels"] = tokenized["input_ids"].copy()

    return tokenized


# ============================================
# MODEL SETUP
# ============================================

def load_model_for_training():
    """Naƒçti model v 4-bit quantization s LoRA"""

    print(f"\nüì• Naƒç√≠t√°m base model: {BASE_MODEL}")
    print("‚ö†Ô∏è  To m≈Ø≈æe trvat nƒõkolik minut...")

    # 4-bit quantization config (QLoRA)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    # Naƒçti model
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    tokenizer.padding_side = 'right'  # D≈Øle≈æit√© pro training

    # P≈ôiprav model pro k-bit training
    model = prepare_model_for_kbit_training(model)

    # P≈ôidej LoRA adapters
    lora_config = LoraConfig(**LORA_CONFIG)
    model = get_peft_model(model, lora_config)

    # Print trainable parameters
    model.print_trainable_parameters()

    print("‚úÖ Model p≈ôipraven pro training!")

    return model, tokenizer


# ============================================
# TRAINING
# ============================================

def train_model(model, tokenizer, dataset):
    """Spus≈• fine-tuning"""

    print("\n" + "="*60)
    print("üöÄ SPOU≈†T√çM FINE-TUNING")
    print("="*60)

    # Form√°tuj dataset
    print("\nüìù Form√°tuji dataset pro Gemma...")
    tokenized_dataset = dataset.map(
        lambda x: format_chat_for_gemma(x, tokenizer),
        remove_columns=dataset["train"].column_names
    )

    # Training arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        **TRAINING_CONFIG,
        fp16=False,              # Gemma 2 pou≈æ√≠v√° bfloat16
        bf16=torch.cuda.is_available(),
        optim="paged_adamw_8bit",  # Optimizer pro QLoRA
        save_total_limit=3,      # Dr≈æ max 3 checkpointy
        push_to_hub=False,       # Zat√≠m nenahr√°v√°me (udƒõl√°me manu√°lnƒõ)
        report_to="none",        # Nebo "wandb" pokud chce≈° tracking
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        tokenizer=tokenizer,
    )

    # Spus≈• training
    print("\n‚è∞ Training zaƒç√≠n√°...")
    print(f"   Poƒçet epoch: {TRAINING_CONFIG['num_train_epochs']}")
    print(f"   Batch size: {TRAINING_CONFIG['per_device_train_batch_size']}")
    print(f"   Learning rate: {TRAINING_CONFIG['learning_rate']}")
    print("\nüí° Training m≈Ø≈æe trvat 1-3 hodiny (z√°le≈æ√≠ na GPU)\n")

    start_time = datetime.now()

    trainer.train()

    end_time = datetime.now()
    duration = end_time - start_time

    print("\n" + "="*60)
    print("‚úÖ TRAINING DOKONƒåEN!")
    print("="*60)
    print(f"‚è±Ô∏è  ƒåas: {duration}")
    print(f"üìÅ Model ulo≈æen do: {OUTPUT_DIR}")

    return trainer


# ============================================
# ULO≈ΩEN√ç & UPLOAD
# ============================================

def save_and_upload(trainer, tokenizer):
    """Ulo≈æ model lok√°lnƒõ a nahraj na HuggingFace"""

    print("\nüíæ Ukl√°d√°m model...")

    # Ulo≈æ LoRA adapters
    trainer.model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    print(f"‚úÖ Model ulo≈æen do: {OUTPUT_DIR}")

    # Volitelnƒõ: Upload na HuggingFace
    upload = input("\nüì§ Chce≈° nahr√°t model na HuggingFace? (y/n): ")

    if upload.lower() == 'y':
        print(f"\nüì§ Nahr√°v√°m na HuggingFace: {HF_REPO_NAME}")
        print("‚ö†Ô∏è  Bude≈° pot≈ôebovat HuggingFace token (zadej p≈ôi v√Ωzvƒõ)")

        from huggingface_hub import login
        login()

        trainer.model.push_to_hub(HF_REPO_NAME)
        tokenizer.push_to_hub(HF_REPO_NAME)

        print(f"‚úÖ Model nahr√°n!")
        print(f"üîó https://huggingface.co/{HF_REPO_NAME}")
    else:
        print("\nüí° Model m≈Ø≈æe≈° nahr√°t pozdƒõji pomoc√≠:")
        print(f"   model.push_to_hub('{HF_REPO_NAME}')")


# ============================================
# MAIN
# ============================================

def main():
    print("\n" + "="*60)
    print("üåø FLEURDIN - FINE-TUNING GEMMA 2-2B-IT")
    print("="*60)

    # 1. Naƒçti dataset
    dataset = prepare_dataset(DATASET_NAME)

    # 2. Naƒçti model
    model, tokenizer = load_model_for_training()

    # 3. Trainuj
    trainer = train_model(model, tokenizer, dataset)

    # 4. Ulo≈æ
    save_and_upload(trainer, tokenizer)

    print("\n" + "="*60)
    print("üéâ HOTOVO!")
    print("="*60)
    print("\nüìã NEXT STEPS:")
    print("1. Otestuj fine-tuned model: python test_finetuned_model.py")
    print("2. Porovnej s base modelem")
    print("3. Deploy na HuggingFace Inference Endpoint")
    print("\n")


if __name__ == "__main__":
    main()
